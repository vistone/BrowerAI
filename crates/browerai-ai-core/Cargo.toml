[package]
name = "browerai-ai-core"
version = "0.1.0"
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "AI core for BrowerAI"

[dependencies]
browerai-core = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }
log = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
toml = { workspace = true }
chrono = { workspace = true }

# Optional Candle runtime for GGUF / quantized LLMs (e.g., Qwen2.5-Coder-7B)
candle-core = { git = "https://github.com/huggingface/candle", optional = true, default-features = false }
# CPU-only: CUDA compilation fails on GTX 1060 (compute 6.1) with current Candle kernels
# For GPU: consider using llama.cpp server + HTTP API instead
candle-transformers = { git = "https://github.com/huggingface/candle", optional = true }
tokenizers = { version = "0.22", optional = true }

[dependencies.ort]
workspace = true
optional = true

[features]
default = []
onnx = ["ort"]
# Legacy alias to keep cfg(feature="ai") code compiling
ai = ["onnx"]
# Enable Candle-based GGUF loading for large language models
candle = ["candle-core", "candle-transformers", "tokenizers"]

[dev-dependencies]
env_logger = { workspace = true }
tempfile = { workspace = true }
