# Training ç›®å½•å¿«é€Ÿå‚è€ƒ

## ğŸ“Š ä¸€è§ˆè¡¨

```
training/                      æ€»è®¡ 116 MB
â”œâ”€â”€ data/                      105 MB    â­ è®­ç»ƒæ•°æ®
â”œâ”€â”€ models/                    8.8 MB    â­ æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ features/                  1.5 MB    ç‰¹å¾æ•°æ®
â”œâ”€â”€ logs/                      672 KB    æ—¥å¿—
â”œâ”€â”€ scripts/                   396 KB    è„šæœ¬ (3,507 è¡Œ)
â”œâ”€â”€ core/                      372 KB    æ¨¡å— (3,116 è¡Œ)
â”œâ”€â”€ semantic_learning/         172 KB    è¯­ä¹‰å­¦ä¹ 
â”œâ”€â”€ venv/                      36 KB     è™šæ‹Ÿç¯å¢ƒ
â””â”€â”€ configs/                   20 KB     é…ç½®
```

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. æ•°æ® (core/data/)
```python
from core.data.tokenizers import Tokenizer
from core.data.website_dataset import WebsiteDataset

# ç¼–ç : å­—ç¬¦çº§ (229 å­—ç¬¦è¡¨)
tokenizer = Tokenizer()
tokens = tokenizer.encode(html_code)

# æ•°æ®é›†: PyTorch å…¼å®¹
dataset = WebsiteDataset(
    data_file='data/website_paired.jsonl',
    tokenizer=tokenizer
)
```

### 2. æ¨¡å‹ (core/models/)
```python
from core.models.website_learner import WebsiteLearner

# æ¶æ„: Transformer Encoder-Decoder
model = WebsiteLearner(
    vocab_size=229,
    d_model=256,
    nhead=8,
    num_layers=3
)

# è¾“å…¥: åŸå§‹ç½‘ç«™ä»£ç 
# è¾“å‡º: ç®€åŒ–ä¼˜åŒ–ç‰ˆæœ¬
output = model(input_tokens)
```

### 3. è®­ç»ƒ (core/trainers/)
```python
from core.trainers.trainer import Trainer

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=30,
    batch_size=2,
    lr=0.001
)
trainer.train()
```

---

## ğŸ“ æ•°æ®é›†ç»“æ„

### è¾“å…¥: åŸå§‹ç½‘ç«™ä»£ç 
```json
{
  "html": "<html>...</html>",
  "css": "body { ... }",
  "js": "function(...) { ... }",
  "url": "https://example.com"
}
```

### è¾“å‡º: ç®€åŒ–ç‰ˆæœ¬
```json
{
  "html_original": "...",
  "html_simplified": "...",  // å‹ç¼©ã€ä¼˜åŒ–
  "css_original": "...",
  "css_simplified": "...",
  "js_original": "...",
  "js_simplified": "..."     // è§£æ··æ·†ã€ç®€åŒ–
}
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ (5 æ­¥)

### 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡
```bash
cd training
pip install -r requirements.txt
```

### 2ï¸âƒ£ æŸ¥çœ‹æ•°æ®
```bash
ls -lh data/websites/
# 142 è¡Œ: 1000_sites.jsonl (å¤§è§„æ¨¡)
# 13 è¡Œ:  quick_train.jsonl (å¿«é€Ÿ)
```

### 3ï¸âƒ£ ç”Ÿæˆé…å¯¹æ•°æ®
```bash
python scripts/create_simplified_dataset.py \
  --input data/website_complete.jsonl \
  --output data/website_paired.jsonl
```

### 4ï¸âƒ£ è®­ç»ƒæ¨¡å‹
```bash
python scripts/train_paired_website_generator.py
# è¾“å‡º: checkpoints/paired_generator/epoch_*.pt
```

### 5ï¸âƒ£ å¯¼å‡º ONNX
```bash
python scripts/export_to_onnx.py \
  --checkpoint checkpoints/paired_generator/epoch_30.pt \
  --output ../models/local/website_generator_v1.onnx
```

---

## ğŸ“‹ è„šæœ¬ä½¿ç”¨åœºæ™¯

| è„šæœ¬ | ç”¨é€” | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| `batch_crawl_websites.py` | çˆ¬å–æ–°ç½‘ç«™ | URLs | websites/*.jsonl |
| `extract_website_complete.py` | å®Œæ•´ç½‘ç«™æå– | åŸå§‹æ•°æ® | website_complete.jsonl |
| `create_simplified_dataset.py` | ç”Ÿæˆç®€åŒ–ç‰ˆ | website_complete.jsonl | website_paired.jsonl |
| `train_paired_website_generator.py` | è®­ç»ƒæ¨¡å‹ | website_paired.jsonl | checkpoints/*.pt |
| `export_to_onnx.py` | å¯¼å‡ºæ¨¡å‹ | checkpoints/*.pt | *.onnx |
| `extract_features.py` | ç‰¹å¾æå– | ä»£ç  | ç‰¹å¾å‘é‡ |
| `dataset_manager.py` | æ•°æ®ç®¡ç† | æ•°æ®æ–‡ä»¶ | ç»Ÿè®¡/éªŒè¯ |

---

## ğŸ” å…³é”®æ–‡ä»¶ä½ç½®

```
training/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_paired_website_generator.py  â­ ä¸»è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ export_to_onnx.py                  â­ ONNX å¯¼å‡º
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ website_learner.py             â­ ä¸»æ¨¡å‹
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ tokenizers.py                  å­—ç¬¦ç¼–ç 
â”‚   â”‚   â””â”€â”€ website_dataset.py             æ•°æ®é›†
â”‚   â””â”€â”€ trainers/
â”‚       â””â”€â”€ trainer.py                     è®­ç»ƒå™¨
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ websites/1000_sites.jsonl          â­ å¤§æ•°æ®é›†
â”‚   â”œâ”€â”€ website_complete.jsonl             å®Œæ•´ç½‘ç«™
â”‚   â””â”€â”€ website_paired.jsonl               é…å¯¹æ•°æ®
â”œâ”€â”€ models/
â”‚   â””â”€â”€ *.onnx.data                        ONNX æ¨¡å‹
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ website_learner.yaml               â­ è®­ç»ƒé…ç½®
â””â”€â”€ README.md, QUICKSTART.md               æ–‡æ¡£
```

---

## âš™ï¸ é…ç½®å‚æ•°

### æ¨¡å‹é…ç½® (website_learner.yaml)
```yaml
# æ¶æ„
model:
  d_model: 256           # åµŒå…¥ç»´åº¦
  nhead: 8               # æ³¨æ„åŠ›å¤´æ•°
  num_layers: 3          # ç¼–ç å™¨/è§£ç å™¨å±‚æ•°
  vocab_size: 229        # å­—ç¬¦è¡¨å¤§å°

# è®­ç»ƒ
training:
  batch_size: 2          # æ‰¹å¤§å°
  epochs: 30             # è®­ç»ƒè½®æ•°
  learning_rate: 0.001   # å­¦ä¹ ç‡
  
# æ•°æ®
data:
  max_length: 5000       # æœ€å¤§åºåˆ—é•¿åº¦
  validation_split: 0.1  # éªŒè¯é›†æ¯”ä¾‹
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### é¢„æœŸæ•ˆæœ (è®­ç»ƒå)
- **BLEU åˆ†æ•°**: 0.70+ (ç›¸ä¼¼åº¦)
- **è¯­æ³•æ­£ç¡®**: 95%+
- **è¯­ä¹‰ä¿ç•™**: 99%+
- **å‹ç¼©ç‡**: 72.95% (ä»£ç é‡)

### è®­ç»ƒæ—¶é—´
- **æ•°æ®è§„æ¨¡**: 13-142 ç½‘ç«™
- **è®­ç»ƒæ—¶é—´**: 2-3 å°æ—¶ (GPU)
- **æ”¶æ•›å‘¨æœŸ**: 20-30 epochs

---

## ğŸ”— é›†æˆåˆ° Rust

### 1. å¯¼å‡ºæ¨¡å‹
```bash
python scripts/export_to_onnx.py \
  --checkpoint checkpoints/paired_generator/epoch_30.pt \
  --output ../models/local/website_generator.onnx
```

### 2. Rust ä¸­ä½¿ç”¨
```rust
use browerai_ai_core::inference::InferenceEngine;

let engine = InferenceEngine::new(
    "models/local/website_generator.onnx"
)?;

let input = tokenizer.encode(original_html)?;
let output = engine.infer(&input)?;
let simplified_html = tokenizer.decode(&output)?;
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q: æ•°æ®æ–‡ä»¶å¤ªå¤§æ€ä¹ˆåŠ?
**A**: æ•°æ®å·²åœ¨ .gitignore ä¸­æ’é™¤ï¼Œåªéœ€æœ¬åœ°ä½¿ç”¨
```bash
# æ•°æ®ä¸ä¼šè¢«æäº¤
git status | grep "websites/"  # åº”è¯¥æ˜¾ç¤º: æ’é™¤çš„æ–‡ä»¶
```

### Q: å¦‚ä½•åŠ è½½å·²è®­ç»ƒæ¨¡å‹?
**A**: 
```python
from core.models.website_learner import WebsiteLearner
model = WebsiteLearner.load('checkpoints/paired_generator/epoch_30.pt')
```

### Q: æ¨¡å‹æ€æ ·éƒ¨ç½²åˆ°ç”Ÿäº§?
**A**: å¯¼å‡ºä¸º ONNXï¼Œé›†æˆåˆ° Rust æ ¸å¿ƒ
```bash
python scripts/export_to_onnx.py --checkpoint ... --output ...
# ç„¶ååœ¨ Rust ä¸­ä½¿ç”¨ InferenceEngine åŠ è½½
```

### Q: å¦‚ä½•æ·»åŠ æ–°çš„ç½‘ç«™æ•°æ®?
**A**:
```bash
# æ–¹æ³•1: ç›´æ¥çˆ¬å–
python scripts/batch_crawl_websites.py --urls-file urls.txt --output ...

# æ–¹æ³•2: æ·»åŠ åˆ°ç°æœ‰æ•°æ®
cat new_sites.jsonl >> data/websites/all_sites.jsonl
```

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

| æ–‡ä»¶ | å†…å®¹ | é€‚åˆäººç¾¤ |
|------|------|---------|
| `README.md` | é¡¹ç›®æ¦‚è§ˆã€æ ¸å¿ƒæ€æƒ³ã€ç›®å½•ç»“æ„ | æ‰€æœ‰äºº |
| `QUICKSTART.md` | åˆ†æ­¥æ•™ç¨‹ã€å®Œæ•´ç¤ºä¾‹ | åˆå­¦è€… |
| `WEBSITE_GENERATION_PLAN.md` | è®¾è®¡ç»†èŠ‚ã€æŠ€æœ¯æ–¹æ¡ˆ | å¼€å‘è€… |
| `TRAINING_DIRECTORY_ANALYSIS.md` | è¯¦ç»†åˆ†æã€æ¨¡å—è¯´æ˜ | æ¶æ„å¸ˆ |
| `core/models/website_learner.py` | æ¨¡å‹å®ç° | ML å·¥ç¨‹å¸ˆ |
| `scripts/train_paired_website_generator.py` | è®­ç»ƒå®ç° | æ•°æ®ç§‘å­¦å®¶ |

---

## ğŸ“ å­¦ä¹ è·¯å¾„

1. **æ–°æ‰‹**: README â†’ QUICKSTART
2. **å¼€å‘è€…**: scripts/ â†’ core/data/ â†’ core/models/
3. **é«˜çº§**: WEBSITE_GENERATION_PLAN â†’ trainer å®ç° â†’ è‡ªå®šä¹‰æ‰©å±•
4. **ç ”ç©¶å‘˜**: è®ºæ–‡åˆ†æ â†’ æ¶æ„æ”¹è¿› â†’ æ€§èƒ½ä¼˜åŒ–

---

## ğŸ“ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# ç¯å¢ƒ
cd training && pip install -r requirements.txt

# æ•°æ®
python scripts/extract_website_complete.py --input ... --output ...
python scripts/create_simplified_dataset.py --input ... --output ...

# è®­ç»ƒ
python scripts/train_paired_website_generator.py

# è¯„ä¼°
python scripts/count_parameters.py  # æ¨¡å‹å¤§å°
python scripts/extract_features.py  # ç‰¹å¾ææ

# å¯¼å‡º
python scripts/export_to_onnx.py --checkpoint ... --output ...

# ç®¡ç†
python scripts/dataset_manager.py --action validate
```

---

**æœ€åæ›´æ–°**: 2026-01-22
**ç‰ˆæœ¬**: 1.0
**ç»´æŠ¤è€…**: BrowerAI Team
