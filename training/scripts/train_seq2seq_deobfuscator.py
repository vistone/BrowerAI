#!/usr/bin/env python3
"""
Seq2Seqå»æ··æ·†æ¨¡å‹ - JavaScriptä»£ç è½¬æ¢
è¾“å…¥: æ··æ·†çš„JSä»£ç 
è¾“å‡º: å»æ··æ·†çš„JSä»£ç 
"""

import sys
import json
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import re
from typing import List, Tuple


class JSTokenizer:
    """JavaScriptè¯æ³•åˆ†æå™¨"""
    
    def __init__(self):
        # ç‰¹æ®Šæ ‡è®°
        self.PAD = '<PAD>'
        self.SOS = '<SOS>'  # Start of sequence
        self.EOS = '<EOS>'  # End of sequence
        self.UNK = '<UNK>'  # Unknown token
        
        # JSå…³é”®å­—
        self.keywords = {
            'const', 'let', 'var', 'function', 'return', 'if', 'else',
            'for', 'while', 'do', 'switch', 'case', 'break', 'continue',
            'class', 'extends', 'constructor', 'this', 'super', 'new',
            'async', 'await', 'promise', 'then', 'catch', 'try', 'finally',
            'import', 'export', 'from', 'default', 'as',
        }
        
        # æ“ä½œç¬¦
        self.operators = {
            '=', '==', '===', '!=', '!==', '<', '>', '<=', '>=',
            '+', '-', '*', '/', '%', '**', '++', '--',
            '&&', '||', '!', '&', '|', '^', '~', '<<', '>>',
            '?', ':', '=>', '.', ',', ';',
        }
        
        # æ„å»ºè¯æ±‡è¡¨
        self.vocab = [self.PAD, self.SOS, self.EOS, self.UNK]
        self.vocab.extend(sorted(self.keywords))
        self.vocab.extend(sorted(self.operators))
        
        # æ·»åŠ æ‹¬å·
        self.vocab.extend(['(', ')', '{', '}', '[', ']'])
        
        # æ·»åŠ å¸¸è§å˜é‡åæ¨¡å¼
        for prefix in ['var', 'tmp', 'val', 'data', 'result', 'item']:
            for i in range(10):
                self.vocab.append(f'{prefix}{i}')
        
        # æ·»åŠ å•å­—æ¯å˜é‡ (æ··æ·†ä»£ç å¸¸è§)
        for c in 'abcdefghijklmnopqrstuvwxyz':
            self.vocab.append(c)
        
        self.vocab2idx = {token: idx for idx, token in enumerate(self.vocab)}
        self.idx2vocab = {idx: token for token, idx in self.vocab2idx.items()}
        self.vocab_size = len(self.vocab)
    
    def tokenize(self, code: str) -> list:
        """å°†ä»£ç åˆ†è¯"""
        tokens = []
        
        # ç®€å•çš„è¯æ³•åˆ†æ
        pattern = r'\w+|[^\w\s]'
        matches = re.findall(pattern, code)
        
        for match in matches:
            if match in self.vocab2idx:
                tokens.append(match)
            elif match in self.keywords:
                tokens.append(match)
            elif len(match) == 1 and match.isalpha():
                tokens.append(match)
            else:
                tokens.append(self.UNK)
        
        return tokens
    
    def encode(self, code: str, max_len=100) -> torch.Tensor:
        """ç¼–ç ä¸ºç´¢å¼•åºåˆ—"""
        tokens = [self.SOS] + self.tokenize(code)[:max_len-2] + [self.EOS]
        indices = [self.vocab2idx.get(token, self.vocab2idx[self.UNK]) for token in tokens]
        return torch.tensor(indices, dtype=torch.long)
    
    def decode(self, indices: torch.Tensor) -> str:
        """è§£ç ä¸ºä»£ç """
        tokens = []
        for idx in indices:
            if idx == self.vocab2idx[self.EOS]:
                break
            if idx == self.vocab2idx[self.PAD]:
                continue
            token = self.idx2vocab.get(idx.item(), self.UNK)
            if token not in [self.SOS, self.PAD]:
                tokens.append(token)
        return ' '.join(tokens)


class Seq2SeqDeobfuscator(nn.Module):
    """Seq2Seqå»æ··æ·†æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # ç¼–ç å™¨
        self.encoder_embed = nn.Embedding(vocab_size, embed_dim)
        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # è§£ç å™¨
        self.decoder_embed = nn.Embedding(vocab_size, embed_dim)
        self.decoder_lstm = nn.LSTM(embed_dim, hidden_dim * 2, batch_first=True)
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim * 2, vocab_size)
    
    def forward(self, src, tgt):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç 
        src_embed = self.encoder_embed(src)
        encoder_output, (hidden, cell) = self.encoder_lstm(src_embed)
        
        # å°†åŒå‘ç¼–ç å™¨çš„éšçŠ¶æ€æ‹¼æ¥ä¸ºè§£ç å™¨åˆå§‹çŠ¶æ€
        # hidden, cell: (2, batch, hidden) -> (1, batch, hidden*2)
        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1).unsqueeze(0)
        cell_cat = torch.cat([cell[0], cell[1]], dim=1).unsqueeze(0)

        # è§£ç 
        tgt_embed = self.decoder_embed(tgt)
        decoder_output, _ = self.decoder_lstm(tgt_embed, (hidden_cat, cell_cat))
        
        # è¾“å‡º
        logits = self.output(decoder_output)
        
        return logits
    
    def generate(self, src, tokenizer, max_len=100):
        """ç”Ÿæˆå»æ··æ·†ä»£ç """
        device = next(self.parameters()).device
        
        # ç¼–ç 
        src_embed = self.encoder_embed(src)
        encoder_output, (hidden, cell) = self.encoder_lstm(src_embed)
        
        # åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€
        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1).unsqueeze(0)
        cell_cat = torch.cat([cell[0], cell[1]], dim=1).unsqueeze(0)
        
        # ä»SOSå¼€å§‹é€tokenç”Ÿæˆ
        batch_size = src.size(0)
        current = torch.tensor([[tokenizer.vocab2idx[tokenizer.SOS]]] * batch_size).to(device)
        generated = []
        
        for _ in range(max_len):
            # è§£ç ä¸€æ­¥
            tgt_embed = self.decoder_embed(current)
            decoder_output, (hidden_cat, cell_cat) = self.decoder_lstm(tgt_embed, (hidden_cat, cell_cat))
            logits = self.output(decoder_output[:, -1, :])
            
            # è´ªå©ªé‡‡æ ·
            next_token = logits.argmax(dim=-1).unsqueeze(1)
            generated.append(next_token.item())
            
            # åˆ¤æ–­ç»“æŸ
            if next_token.item() == tokenizer.vocab2idx[tokenizer.EOS]:
                break
            
            current = next_token
        
        return torch.tensor(generated)
    
    def inference(self, src, max_len=60):
        """ONNXæ¨ç†æ¨¡å¼ï¼šç¼–ç å™¨+è§£ç å™¨å®Œæ•´å‰å‘ï¼Œè¿”å›token ids"""
        device = next(self.parameters()).device
        batch_size = src.size(0)
        
        # ç¼–ç 
        src_embed = self.encoder_embed(src)
        encoder_output, (hidden, cell) = self.encoder_lstm(src_embed)
        
        # åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€
        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1).unsqueeze(0)
        cell_cat = torch.cat([cell[0], cell[1]], dim=1).unsqueeze(0)
        
        # ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼ˆå›ºå®šé•¿åº¦ï¼Œç”¨äºONNXå¯¼å‡ºï¼‰
        outputs = []
        current = torch.full((batch_size, 1), 1, dtype=torch.long).to(device)  # SOS token = 1
        
        for _ in range(max_len):
            tgt_embed = self.decoder_embed(current)
            decoder_output, (hidden_cat, cell_cat) = self.decoder_lstm(tgt_embed, (hidden_cat, cell_cat))
            logits = self.output(decoder_output[:, -1, :])
            next_token = logits.argmax(dim=-1, keepdim=True)
            outputs.append(next_token)
            current = next_token
        
        # æ‹¼æ¥æ‰€æœ‰è¾“å‡º [batch, max_len]
        return torch.cat(outputs, dim=1)


class InferenceWrapper(nn.Module):
    """ONNXå¯¼å‡ºåŒ…è£…å™¨"""
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, src):
        return self.model.inference(src, max_len=60)


def create_synthetic_dataset(tokenizer, num_samples=1000):
    """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®
    ç”±äºæ²¡æœ‰çœŸå®çš„æ··æ·†-åŸå§‹ä»£ç å¯¹ï¼Œæˆ‘ä»¬ç”Ÿæˆç®€å•çš„åˆæˆæ•°æ®
    å®é™…ä½¿ç”¨ä¸­éœ€è¦çœŸå®çš„é…å¯¹æ•°æ®
    """
    samples = []
    
    patterns = [
        # æ¨¡å¼1: å˜é‡é‡å‘½å
        ('const a = 5;', 'const value = 5;'),
        ('let b = "hello";', 'let message = "hello";'),
        ('var c = true;', 'var isActive = true;'),
        
        # æ¨¡å¼2: å‡½æ•°ç®€åŒ–
        ('function a(b){return b*2}', 'function double(value){return value*2}'),
        ('const b=c=>c+1', 'const increment = value => value+1'),
        
        # æ¨¡å¼3: é€»è¾‘ç®€åŒ–
        ('if(a){b}else{c}', 'if(condition){doA}else{doB}'),
        ('a?b:c', 'condition?trueValue:falseValue'),
    ]
    
    # ç”Ÿæˆå˜ä½“
    for obfuscated, clean in patterns:
        for i in range(num_samples // len(patterns)):
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            samples.append((obfuscated, clean))
    
    return samples


class PairedCodeDataset(Dataset):
    """æ··æ·†-åŸå§‹æˆå¯¹æ•°æ®é›†"""

    def __init__(self, pairs: List[Tuple[str, str]], tokenizer: JSTokenizer, max_len: int = 120):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        obf, clean = self.pairs[idx]
        src = self.tokenizer.encode(obf, max_len=self.max_len)
        tgt = self.tokenizer.encode(clean, max_len=self.max_len)
        return src, tgt


def collate_batch(batch):
    src_batch, tgt_batch = zip(*batch)
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    return src_padded, tgt_padded


def main():
    print("=" * 60)
    print("ğŸš€ Seq2Seqå»æ··æ·†æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # 1. åˆ›å»ºtokenizer
    tokenizer = JSTokenizer()
    print(f"ğŸ“– è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # 2. è¯»å–çœŸå®é…å¯¹æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åˆæˆæ•°æ®
    pairs_path = Path(__file__).parent.parent / 'data' / 'obfuscation_pairs.jsonl'
    pairs: List[Tuple[str, str]] = []
    if pairs_path.exists():
        print(f"ğŸ“‚ å‘ç°çœŸå®é…å¯¹æ•°æ®: {pairs_path}")
        with open(pairs_path) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    item = json.loads(line)
                    obf = item.get('obfuscated') or item.get('obf')
                    clean = item.get('clean') or item.get('original')
                    if obf and clean:
                        pairs.append((obf, clean))
                except Exception:
                    continue
    if not pairs:
        print("âš ï¸  æœªæ‰¾åˆ°çœŸå®é…å¯¹æ•°æ®ï¼Œä½¿ç”¨åˆæˆæ ·æœ¬æ¼”ç¤º")
        pairs = create_synthetic_dataset(tokenizer, num_samples=200)

    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(pairs)}")
    dataset = PairedCodeDataset(pairs, tokenizer)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = Seq2SeqDeobfuscator(
        vocab_size=tokenizer.vocab_size,
        embed_dim=128,
        hidden_dim=256
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹å‚æ•°: {total_params:,}")

    # 4. DataLoader
    loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_batch)

    # 5. è®­ç»ƒï¼ˆè½»é‡çº§ç¤ºä¾‹ï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
    model = model.to(device)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    epochs = 3
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        for src, tgt in loader:
            src = src.to(device)
            tgt = tgt.to(device)

            optimizer.zero_grad()
            # æ•™å¸ˆå¼ºåˆ¶ï¼šè¾“å…¥æ˜¯ tgt[:-1]ï¼Œé¢„æµ‹ tgt[1:]
            logits = model(src, tgt[:, :-1])
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),
                tgt[:, 1:].reshape(-1)
            )
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / max(len(loader), 1)
        print(f"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}")

    # 6. ç®€å•æ¨ç†æ¼”ç¤º
    model.eval()
    sample_src, sample_tgt = dataset[0]
    gen_indices = model.generate(sample_src.unsqueeze(0).to(device), tokenizer, max_len=60)
    generated = tokenizer.decode(gen_indices.cpu())
    print("\nğŸ§ª ç”Ÿæˆç¤ºä¾‹ï¼š")
    print("æ··æ·†:", tokenizer.decode(sample_src))
    print("æœŸæœ›:", tokenizer.decode(sample_tgt))
    print("ç”Ÿæˆ:", generated)

    # 7. ä¿å­˜tokenizeré…ç½®
    config_path = Path(__file__).parent.parent / 'models' / 'tokenizer_config.json'
    config = {
        'vocab': tokenizer.vocab,
        'vocab_size': tokenizer.vocab_size,
    }
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nğŸ’¾ Tokenizeré…ç½®å·²ä¿å­˜: {config_path}")

    # 8. å¯¼å‡ºONNXï¼ˆä½¿ç”¨inferenceåŒ…è£…å™¨ï¼‰
    models_dir = Path(__file__).parent.parent / 'models'
    models_dir.mkdir(exist_ok=True)
    onnx_path = models_dir / 'js_deobfuscator_v1.onnx'

    model.eval()
    wrapper = InferenceWrapper(model)
    dummy_src = torch.randint(0, tokenizer.vocab_size, (1, 60)).to(device)

    # å¯¼å‡ºåŒ…è£…æ¨¡å‹
    torch.onnx.export(
        wrapper,
        dummy_src,
        str(onnx_path),
        input_names=['src'],
        output_names=['output'],
        opset_version=13,
        do_constant_folding=True,
        export_params=True,
        dynamic_axes={
            'src': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        },
    )

    print(f"ğŸ’¾ ONNX å·²å¯¼å‡º: {onnx_path} (ä½¿ç”¨inferenceæ¨¡å¼)")

    print("\nâš ï¸  æ³¨æ„: è¦å–å¾—çœŸå®æ•ˆæœï¼Œè¯·å‡†å¤‡1ä¸‡+çœŸå®æ··æ·†â†”åŸå§‹é…å¯¹æ ·æœ¬ï¼Œå¹¶å¯æ”¹ç”¨Transformeræ¶æ„")
    print("   æœ¬è„šæœ¬æ”¯æŒï¼šå°†çœŸå®é…å¯¹æ•°æ®æ”¾åˆ° training/data/obfuscation_pairs.jsonl (æ¯è¡Œ {obfuscated, clean})")
    print("\nğŸ“š å»ºè®®:")
    print("   1. ä½¿ç”¨ uglify/terser/obfuscator-js å¯¹å¼€æºJSåšæ··æ·†ï¼Œä¿ç•™åŸå§‹ä»£ç ä½œæ ‡ç­¾")
    print("   2. å¢åŠ æ ·æœ¬è¦†ç›–äº‹ä»¶å¾ªç¯/å¼‚æ­¥/æ¨¡å—åŒ–ç­‰å¤šæ¨¡å¼")
    print("   3. æ¢ç”¨ Transformer (å°å‹) æ›¿ä»£ LSTM æå‡è¡¨ç°")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
