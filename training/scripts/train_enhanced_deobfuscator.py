#!/usr/bin/env python3
"""
å¢å¼ºå‹ JS å»æ··æ·†è®­ç»ƒè„šæœ¬
ä½¿ç”¨å¯¹æŠ—è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ æå‡å»æ··æ·†èƒ½åŠ›

ç‰¹æ€§:
1. è‡ªåŠ¨ç”Ÿæˆå¤šç§æ··æ·†æ ·æœ¬
2. ä½¿ç”¨å¯¹æŠ—å­¦ä¹ æå‡é²æ£’æ€§
3. æ”¯æŒå¤šç§æ··æ·†æŠ€æœ¯è¯†åˆ«
4. æ¸è¿›å¼å»æ··æ·†ç­–ç•¥
"""

import sys
import json
import random
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Dict


class ObfuscationTechnique:
    """æ··æ·†æŠ€æœ¯"""
    
    @staticmethod
    def name_mangle(code: str) -> str:
        """å˜é‡åæ··æ·†"""
        import re
        
        # ç®€å•çš„å˜é‡åæ›¿æ¢
        var_names = re.findall(r'\b[a-zA-Z_]\w+\b', code)
        unique_vars = set(var_names)
        
        mapping = {}
        for i, var in enumerate(unique_vars):
            if var not in ['const', 'let', 'var', 'function', 'return', 'if', 'else', 'for', 'while']:
                mapping[var] = chr(97 + (i % 26))  # a-z
        
        result = code
        for old, new in mapping.items():
            result = re.sub(r'\b' + old + r'\b', new, result)
        
        return result
    
    @staticmethod
    def string_encode(code: str) -> str:
        """å­—ç¬¦ä¸²ç¼–ç """
        import re
        
        def encode_string(match):
            s = match.group(1)
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶ç¼–ç 
            hex_str = ''.join([f'\\x{ord(c):02x}' for c in s])
            return f'"{hex_str}"'
        
        return re.sub(r'"([^"]+)"', encode_string, code)
    
    @staticmethod
    def whitespace_remove(code: str) -> str:
        """å»é™¤ç©ºç™½"""
        return ' '.join(code.split())
    
    @staticmethod
    def dead_code_inject(code: str) -> str:
        """æ³¨å…¥æ­»ä»£ç """
        dead_code = [
            'if (false) { console.log("dead"); }',
            'while (false) { break; }',
            'var unused = 0;',
        ]
        
        lines = code.split('\n')
        if len(lines) > 1:
            insert_pos = random.randint(0, len(lines))
            lines.insert(insert_pos, random.choice(dead_code))
        
        return '\n'.join(lines)
    
    @staticmethod
    def apply_all(code: str) -> str:
        """åº”ç”¨æ‰€æœ‰æ··æ·†æŠ€æœ¯"""
        code = ObfuscationTechnique.name_mangle(code)
        code = ObfuscationTechnique.string_encode(code)
        code = ObfuscationTechnique.whitespace_remove(code)
        code = ObfuscationTechnique.dead_code_inject(code)
        return code


class EnhancedDeobfuscator(nn.Module):
    """å¢å¼ºå‹å»æ··æ·†æ¨¡å‹ï¼ˆä½¿ç”¨ Transformerï¼‰"""
    
    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 8,
                 num_layers: int = 6, dim_feedforward: int = 1024):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(self.pos_encoder, num_layers)
        
        # è§£ç å±‚
        self.decoder = nn.Linear(d_model, vocab_size)
        
        # æ··æ·†æ£€æµ‹å™¨ï¼ˆè¾…åŠ©ä»»åŠ¡ï¼‰
        self.obfuscation_detector = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Linear(128, 5)  # 5 ç§æ··æ·†æŠ€æœ¯
        )
    
    def forward(self, src):
        """å‰å‘ä¼ æ’­"""
        # åµŒå…¥
        x = self.embedding(src)
        
        # Transformer ç¼–ç 
        encoded = self.transformer(x)
        
        # è§£ç 
        output = self.decoder(encoded)
        
        # æ··æ·†æ£€æµ‹ï¼ˆä½¿ç”¨å¹³å‡æ± åŒ–ï¼‰
        obf_features = encoded.mean(dim=1)
        obf_logits = self.obfuscation_detector(obf_features)
        
        return output, obf_logits


class ObfuscationDataset(Dataset):
    """æ··æ·†æ•°æ®é›†ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰"""
    
    def __init__(self, clean_code_samples: List[str], tokenizer, max_len: int = 150):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.samples = []
        
        # ä¸ºæ¯ä¸ªå¹²å‡€ä»£ç ç”Ÿæˆå¤šç§æ··æ·†ç‰ˆæœ¬
        for clean_code in clean_code_samples:
            # åŸå§‹ï¼ˆæ— æ··æ·†ï¼‰
            self.samples.append((clean_code, clean_code, [0, 0, 0, 0, 0]))
            
            # å˜é‡åæ··æ·†
            obf1 = ObfuscationTechnique.name_mangle(clean_code)
            self.samples.append((obf1, clean_code, [1, 0, 0, 0, 0]))
            
            # å­—ç¬¦ä¸²ç¼–ç 
            obf2 = ObfuscationTechnique.string_encode(clean_code)
            self.samples.append((obf2, clean_code, [0, 1, 0, 0, 0]))
            
            # å»é™¤ç©ºç™½
            obf3 = ObfuscationTechnique.whitespace_remove(clean_code)
            self.samples.append((obf3, clean_code, [0, 0, 1, 0, 0]))
            
            # ç»¼åˆæ··æ·†
            obf_all = ObfuscationTechnique.apply_all(clean_code)
            self.samples.append((obf_all, clean_code, [1, 1, 1, 1, 0]))
        
        print(f"Generated {len(self.samples)} training pairs")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        obfuscated, clean, obf_labels = self.samples[idx]
        
        src = self.tokenizer.encode(obfuscated, self.max_len)
        tgt = self.tokenizer.encode(clean, self.max_len)
        obf_tensor = torch.tensor(obf_labels, dtype=torch.float32)
        
        return src, tgt, obf_tensor


def generate_clean_code_samples() -> List[str]:
    """ç”Ÿæˆå¹²å‡€çš„ä»£ç æ ·æœ¬"""
    samples = [
        # ç®€å•å‡½æ•°
        "function add(a, b) { return a + b; }",
        "function multiply(x, y) { return x * y; }",
        "function greet(name) { return 'Hello, ' + name; }",
        
        # å˜é‡å£°æ˜
        "const message = 'Hello World';",
        "let count = 0;",
        "var isActive = true;",
        
        # æ§åˆ¶æµ
        "if (condition) { doSomething(); } else { doOther(); }",
        "for (let i = 0; i < 10; i++) { console.log(i); }",
        "while (running) { update(); }",
        
        # å¯¹è±¡å’Œæ•°ç»„
        "const user = { name: 'John', age: 30 };",
        "const numbers = [1, 2, 3, 4, 5];",
        
        # å¼‚æ­¥æ“ä½œ
        "async function fetchData() { const response = await fetch(url); return response.json(); }",
        "promise.then(result => console.log(result)).catch(error => console.error(error));",
        
        # ç±»
        "class Person { constructor(name) { this.name = name; } greet() { return 'Hi!'; } }",
        
        # ç®­å¤´å‡½æ•°
        "const double = x => x * 2;",
        "const sum = (a, b) => a + b;",
    ]
    
    return samples


def main():
    print("=" * 60)
    print("ğŸš€ å¢å¼ºå‹ JS å»æ··æ·†æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("ğŸ“ ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    clean_samples = generate_clean_code_samples()
    
    # ä½¿ç”¨ç®€å•çš„ tokenizer
    from train_seq2seq_deobfuscator import JSTokenizer
    tokenizer = JSTokenizer()
    print(f"ğŸ“– è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = ObfuscationDataset(clean_samples, tokenizer)
    loader = DataLoader(dataset, batch_size=16, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = EnhancedDeobfuscator(
        vocab_size=tokenizer.vocab_size,
        d_model=256,
        nhead=8,
        num_layers=4,
        dim_feedforward=1024
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # è®­ç»ƒè®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
    
    model = model.to(device)
    
    # ä¸¤ä¸ªæŸå¤±å‡½æ•°
    criterion_deobf = nn.CrossEntropyLoss(ignore_index=0)
    criterion_detect = nn.BCEWithLogitsLoss()
    
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    # è®­ç»ƒ
    epochs = 10
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        total_deobf_loss = 0.0
        total_detect_loss = 0.0
        
        for src, tgt, obf_labels in loader:
            src = src.to(device)
            tgt = tgt.to(device)
            obf_labels = obf_labels.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output, obf_logits = model(src)
            
            # å»æ··æ·†æŸå¤±
            deobf_loss = criterion_deobf(
                output.reshape(-1, output.size(-1)),
                tgt.reshape(-1)
            )
            
            # æ··æ·†æ£€æµ‹æŸå¤±
            detect_loss = criterion_detect(obf_logits, obf_labels)
            
            # æ€»æŸå¤±ï¼ˆåŠ æƒï¼‰
            loss = deobf_loss + 0.1 * detect_loss
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            total_deobf_loss += deobf_loss.item()
            total_detect_loss += detect_loss.item()
        
        avg_loss = total_loss / len(loader)
        avg_deobf = total_deobf_loss / len(loader)
        avg_detect = total_detect_loss / len(loader)
        
        print(f"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f} "
              f"(Deobf: {avg_deobf:.4f}, Detect: {avg_detect:.4f})")
    
    # æµ‹è¯•
    print("\nğŸ§ª æµ‹è¯•å»æ··æ·†:")
    model.eval()
    
    test_cases = [
        ("ç®€å•æ··æ·†", "function a(b){return b*2}"),
        ("å­—ç¬¦ä¸²ç¼–ç ", r'const msg="\x48\x65\x6c\x6c\x6f";'),
        ("ç»¼åˆæ··æ·†", "function a(b,c){var d=0;for(var e=0;e<10;e++){d+=e}return d}"),
    ]
    
    for name, obfuscated in test_cases:
        print(f"\n{name}:")
        print(f"  è¾“å…¥: {obfuscated}")
        
        src = tokenizer.encode(obfuscated, max_len=150).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output, obf_logits = model(src)
            
            # è§£ç è¾“å‡º
            predicted_ids = output.argmax(dim=-1)
            deobfuscated = tokenizer.decode(predicted_ids[0].cpu())
            
            # æ··æ·†æ£€æµ‹
            obf_probs = torch.sigmoid(obf_logits[0]).cpu().numpy()
            techniques = ['NameMangle', 'StringEncode', 'Whitespace', 'DeadCode', 'Other']
            detected = [techniques[i] for i, p in enumerate(obf_probs) if p > 0.5]
            
            print(f"  è¾“å‡º: {deobfuscated[:80]}...")
            print(f"  æ£€æµ‹: {', '.join(detected) if detected else 'None'}")
    
    # å¯¼å‡ºæ¨¡å‹
    models_dir = Path(__file__).parent.parent / 'models'
    models_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ PyTorch æ¨¡å‹
    torch_path = models_dir / 'enhanced_deobfuscator_v1.pth'
    torch.save(model.state_dict(), torch_path)
    print(f"\nğŸ’¾ PyTorch æ¨¡å‹å·²ä¿å­˜: {torch_path}")
    
    # å¯¼å‡º ONNXï¼ˆä»…å»æ··æ·†éƒ¨åˆ†ï¼‰
    onnx_path = models_dir / 'enhanced_deobfuscator_v1.onnx'
    model.eval()
    dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 60)).to(device)
    
    # åˆ›å»ºåªè¾“å‡ºå»æ··æ·†ç»“æœçš„åŒ…è£…å™¨
    class DeobfuscatorWrapper(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
        
        def forward(self, src):
            output, _ = self.model(src)
            return output.argmax(dim=-1)
    
    wrapper = DeobfuscatorWrapper(model)
    
    torch.onnx.export(
        wrapper,
        dummy_input,
        str(onnx_path),
        input_names=['input'],
        output_names=['output'],
        opset_version=13,
        dynamic_axes={
            'input': {0: 'batch_size', 1: 'sequence'},
            'output': {0: 'batch_size', 1: 'sequence'}
        }
    )
    
    print(f"ğŸ’¾ ONNX æ¨¡å‹å·²å¯¼å‡º: {onnx_path}")
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print("\nç‰¹æ€§:")
    print("  âœ“ å¤šä»»åŠ¡å­¦ä¹ ï¼ˆå»æ··æ·† + æ··æ·†æ£€æµ‹ï¼‰")
    print("  âœ“ æ”¯æŒ 5 ç§æ··æ·†æŠ€æœ¯è¯†åˆ«")
    print("  âœ“ Transformer æ¶æ„å¢å¼ºç†è§£èƒ½åŠ›")
    print("  âœ“ è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æ”¶é›†çœŸå®æ··æ·†æ ·æœ¬æ‰©å±•è®­ç»ƒé›†")
    print("  2. è°ƒæ•´æ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°")
    print("  3. æ·»åŠ æ›´å¤šæ··æ·†æŠ€æœ¯æ”¯æŒ")
    print("  4. å®æ–½å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å»æ··æ·†ç­–ç•¥")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
