#!/usr/bin/env python3
"""
å¤§è§„æ¨¡ç½‘ç«™æ‰¹é‡çˆ¬å–è„šæœ¬

ç”¨äºçˆ¬å–1000+ç½‘ç«™æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é”™è¯¯é‡è¯•ã€è¿›åº¦ä¿å­˜
"""

import asyncio
import aiohttp
import json
import sys
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import logging

# å¯¼å…¥ç°æœ‰çš„çˆ¬è™«æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from prepare_website_data import WebsiteCrawler, get_example_urls

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('large_crawl.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class LargeScaleCrawler:
    """å¤§è§„æ¨¡ç½‘ç«™çˆ¬å–å™¨"""
    
    def __init__(self, urls_file: Path, output_dir: Path, 
                 batch_size: int = 50, max_depth: int = 2, max_pages: int = 5):
        self.urls_file = urls_file
        self.output_dir = output_dir
        self.batch_size = batch_size
        self.max_depth = max_depth
        self.max_pages = max_pages
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress_file = output_dir / "crawl_progress.json"
        self.completed_urls = self.load_progress()
        
    def load_progress(self) -> set:
        """åŠ è½½å·²å®Œæˆçš„URLï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                    logger.info(f"ğŸ“‚ åŠ è½½è¿›åº¦: å·²å®Œæˆ {len(data['completed'])} ä¸ªç½‘ç«™")
                    return set(data['completed'])
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½è¿›åº¦æ–‡ä»¶: {e}")
        return set()
    
    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        try:
            with open(self.progress_file, 'w') as f:
                json.dump({
                    'completed': list(self.completed_urls),
                    'total': len(self.completed_urls),
                    'last_update': datetime.now().isoformat()
                }, f, indent=2)
        except Exception as e:
            logger.error(f"âŒ æ— æ³•ä¿å­˜è¿›åº¦: {e}")
    
    def load_urls(self) -> list:
        """åŠ è½½URLåˆ—è¡¨"""
        urls = []
        with open(self.urls_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split(',')
                    url = parts[0]
                    category = parts[1] if len(parts) > 1 else 'unknown'
                    
                    # è·³è¿‡å·²å®Œæˆçš„URL
                    if url not in self.completed_urls:
                        urls.append((url, category))
        
        logger.info(f"ğŸ“‹ å¾…çˆ¬å–: {len(urls)} ä¸ªç½‘ç«™ (å·²å®Œæˆ: {len(self.completed_urls)})")
        return urls
    
    async def crawl_batch(self, batch: list, batch_num: int, total_batches: int):
        """çˆ¬å–ä¸€æ‰¹ç½‘ç«™"""
        batch_file = self.output_dir / f"batch_{batch_num:04d}.jsonl"
        
        logger.info(f"\n{'='*60}")
        logger.info(f"å¼€å§‹æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªç½‘ç«™)")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {batch_file}")
        logger.info(f"{'='*60}\n")
        
        # åˆ›å»ºçˆ¬è™«
        crawler = WebsiteCrawler(
            max_files=50,
            max_depth=self.max_depth,
            max_pages=self.max_pages
        )
        
        # çˆ¬å–ç»“æœ
        results = []
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for url, category in tqdm(batch, desc=f"æ‰¹æ¬¡ {batch_num}"):
            try:
                logger.info(f"ğŸŒ çˆ¬å–: {url}")
                data = await crawler.crawl_website_with_depth(url, category)
                
                if data:
                    results.append(data)
                    self.completed_urls.add(url)
                    
                    # å®æ—¶ç»Ÿè®¡
                    pages = data.get('depth', 1)
                    framework = data.get('metadata', {}).get('framework', 'Unknown')
                    logger.info(f"âœ… å®Œæˆ: {url} - {pages}é¡µ, {framework}")
                else:
                    logger.warning(f"âš ï¸ ç©ºæ•°æ®: {url}")
                    
            except Exception as e:
                logger.error(f"âŒ å¤±è´¥: {url} - {e}")
                continue
            
            # æ¯10ä¸ªç½‘ç«™ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if len(results) % 10 == 0:
                self.save_progress()
        
        # ä¿å­˜æ‰¹æ¬¡ç»“æœ
        if results:
            with open(batch_file, 'w', encoding='utf-8') as f:
                for item in results:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            logger.info(f"\nâœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(results)}/{len(batch)} ä¸ªç½‘ç«™")
        
        # ä¿å­˜è¿›åº¦
        self.save_progress()
        
        return len(results)
    
    async def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰ç½‘ç«™"""
        urls = self.load_urls()
        
        if not urls:
            logger.info("ğŸ‰ æ‰€æœ‰ç½‘ç«™å·²çˆ¬å–å®Œæˆï¼")
            return
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (len(urls) + self.batch_size - 1) // self.batch_size
        total_crawled = 0
        
        logger.info(f"\nğŸ“Š çˆ¬å–è®¡åˆ’:")
        logger.info(f"  - æ€»ç½‘ç«™æ•°: {len(urls)}")
        logger.info(f"  - æ‰¹æ¬¡æ•°é‡: {total_batches}")
        logger.info(f"  - æ¯æ‰¹å¤§å°: {self.batch_size}")
        logger.info(f"  - æ·±åº¦è®¾ç½®: {self.max_depth}")
        logger.info(f"  - æœ€å¤§é¡µé¢: {self.max_pages}")
        logger.info(f"  - é¢„è®¡é¡µé¢: ~{len(urls) * 3} é¡µ")
        logger.info(f"\n")
        
        for i in range(0, len(urls), self.batch_size):
            batch = urls[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            
            try:
                count = await self.crawl_batch(batch, batch_num, total_batches)
                total_crawled += count
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼ˆé¿å…è¢«å°IPï¼‰
                if batch_num < total_batches:
                    logger.info(f"ğŸ˜´ ä¼‘æ¯ 30 ç§’...")
                    await asyncio.sleep(30)
                    
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} é”™è¯¯: {e}")
                continue
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ!")
        logger.info(f"  - æˆåŠŸçˆ¬å–: {total_crawled} ä¸ªç½‘ç«™")
        logger.info(f"  - æ€»è®¡å®Œæˆ: {len(self.completed_urls)} ä¸ªç½‘ç«™")
        logger.info(f"  - è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"{'='*60}\n")
    
    def merge_batches(self, output_file: Path):
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶"""
        logger.info(f"\nğŸ“¦ åˆå¹¶æ‰¹æ¬¡æ–‡ä»¶åˆ°: {output_file}")
        
        batch_files = sorted(self.output_dir.glob("batch_*.jsonl"))
        total_sites = 0
        total_pages = 0
        
        with open(output_file, 'w', encoding='utf-8') as out:
            for batch_file in batch_files:
                with open(batch_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            out.write(line)
                            data = json.loads(line)
                            total_sites += 1
                            total_pages += data.get('depth', 1)
        
        logger.info(f"âœ… åˆå¹¶å®Œæˆ:")
        logger.info(f"  - ç½‘ç«™æ€»æ•°: {total_sites}")
        logger.info(f"  - é¡µé¢æ€»æ•°: {total_pages}")
        logger.info(f"  - å¹³å‡æ·±åº¦: {total_pages/total_sites:.1f} é¡µ/ç«™")
        logger.info(f"  - è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        return total_sites, total_pages


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤§è§„æ¨¡ç½‘ç«™æ‰¹é‡çˆ¬å–")
    parser.add_argument(
        "--urls-file",
        type=Path,
        default=Path("data/large_urls.txt"),
        help="URLåˆ—è¡¨æ–‡ä»¶"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("data/websites/large_scale"),
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="æ¯æ‰¹çˆ¬å–çš„ç½‘ç«™æ•°é‡"
    )
    parser.add_argument(
        "--depth",
        type=int,
        default=2,
        help="çˆ¬å–æ·±åº¦"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=5,
        help="æ¯ä¸ªç½‘ç«™æœ€å¤§é¡µé¢æ•°"
    )
    parser.add_argument(
        "--merge",
        action="store_true",
        help="åªåˆå¹¶å·²æœ‰çš„æ‰¹æ¬¡æ–‡ä»¶"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/websites/large_train.jsonl"),
        help="åˆå¹¶åçš„è¾“å‡ºæ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«
    crawler = LargeScaleCrawler(
        urls_file=args.urls_file,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        max_depth=args.depth,
        max_pages=args.max_pages
    )
    
    if args.merge:
        # åªåˆå¹¶æ–‡ä»¶
        crawler.merge_batches(args.output)
    else:
        # çˆ¬å– + åˆå¹¶
        await crawler.crawl_all()
        crawler.merge_batches(args.output)
    
    logger.info("\nâœ¨ ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())
