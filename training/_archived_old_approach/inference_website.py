#!/usr/bin/env python3
"""
ç½‘ç«™ç”Ÿæˆä¸æ¨ç†è„šæœ¬

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œ:
1. ç½‘ç«™åˆ†ç±»é¢„æµ‹
2. æ¡†æ¶è¯†åˆ«
3. é£æ ¼åˆ†æ
4. ç›¸ä¼¼ç½‘ç«™æ¨è
"""

import torch
import torch.nn.functional as F
from pathlib import Path
import json
import sys
from typing import Dict, List
import logging

sys.path.insert(0, str(Path(__file__).parent.parent))

from core.data.tokenizers import CodeTokenizer
from core.models.website_learner import HolisticWebsiteLearner
from core.data.website_dataset import WebsiteDataset

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WebsiteInference:
    """ç½‘ç«™æ¨ç†å¼•æ“"""
    
    CATEGORIES = WebsiteDataset.CATEGORIES
    FRAMEWORKS = ["React", "Vue", "Angular", "jQuery", "Svelte", "Tailwind", "Bootstrap", "Unknown"]
    
    def __init__(self, model_path: Path, vocab_size: int = 10000):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ–¥ï¸  æ¨ç†è®¾å¤‡: {self.device}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = CodeTokenizer(vocab_size=vocab_size)
        
        # åŠ è½½æ¨¡å‹
        self.model = self.load_model(model_path)
        self.model.eval()
        
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_model(self, model_path: Path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})
        
        # åˆ›å»ºæ¨¡å‹
        model = HolisticWebsiteLearner(
            vocab_size=config.get('vocab_size', 10000),
            d_model=config.get('d_model', 512),
            nhead=config.get('nhead', 8),
            num_layers=config.get('num_layers', 6),
            num_categories=len(self.CATEGORIES),
            url_feature_dim=128
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        
        return model
    
    def extract_url_features(self, url: str) -> torch.Tensor:
        """æå–URLç‰¹å¾"""
        features = [0.0] * 128
        url_hash = hash(url) % 128
        features[url_hash] = 1.0
        
        if ".com" in url:
            features[0] = 1.0
        if ".org" in url:
            features[1] = 1.0
        if ".net" in url:
            features[2] = 1.0
        if ".edu" in url:
            features[3] = 1.0
        if ".gov" in url:
            features[4] = 1.0
        
        return torch.tensor(features, dtype=torch.float32)
    
    def preprocess_website(self, html: str, css: str, js: str, url: str) -> Dict[str, torch.Tensor]:
        """é¢„å¤„ç†ç½‘ç«™æ•°æ®"""
        # åˆ†è¯
        html_ids = self.tokenizer.encode(html, max_length=2048)
        css_ids = self.tokenizer.encode(css, max_length=1024)
        js_ids = self.tokenizer.encode(js, max_length=2048)
        
        # å¡«å……/æˆªæ–­
        html_ids = html_ids + [0] * (2048 - len(html_ids))
        css_ids = css_ids + [0] * (1024 - len(css_ids))
        js_ids = js_ids + [0] * (2048 - len(js_ids))
        
        html_ids = html_ids[:2048]
        css_ids = css_ids[:1024]
        js_ids = js_ids[:2048]
        
        # URLç‰¹å¾
        url_features = self.extract_url_features(url)
        
        return {
            'html_ids': torch.tensor([html_ids], dtype=torch.long),
            'css_ids': torch.tensor([css_ids], dtype=torch.long),
            'js_ids': torch.tensor([js_ids], dtype=torch.long),
            'url_features': url_features.unsqueeze(0)
        }
    
    @torch.no_grad()
    def predict(self, html: str, css: str, js: str, url: str) -> Dict:
        """é¢„æµ‹ç½‘ç«™å±æ€§"""
        # é¢„å¤„ç†
        inputs = self.preprocess_website(html, css, js, url)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        html_ids = inputs['html_ids'].to(self.device)
        css_ids = inputs['css_ids'].to(self.device)
        js_ids = inputs['js_ids'].to(self.device)
        url_features = inputs['url_features'].to(self.device)
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(html_ids, css_ids, js_ids, url_features)
        
        # ç±»åˆ«é¢„æµ‹
        category_probs = F.softmax(outputs['category_logits'], dim=1)[0]
        category_idx = category_probs.argmax().item()
        category_conf = category_probs[category_idx].item()
        
        # Top-3ç±»åˆ«
        top3_categories = []
        top3_probs, top3_indices = category_probs.topk(3)
        for prob, idx in zip(top3_probs, top3_indices):
            top3_categories.append({
                'category': self.CATEGORIES[idx.item()],
                'confidence': prob.item()
            })
        
        # æ¡†æ¶é¢„æµ‹
        framework_probs = F.softmax(outputs['framework_logits'], dim=1)[0]
        framework_idx = framework_probs.argmax().item()
        framework_conf = framework_probs[framework_idx].item()
        
        # é£æ ¼åµŒå…¥
        style_embedding = outputs['style_embedding'][0].cpu().numpy()
        
        return {
            'category': self.CATEGORIES[category_idx],
            'category_confidence': category_conf,
            'top3_categories': top3_categories,
            'framework': self.FRAMEWORKS[min(framework_idx, len(self.FRAMEWORKS)-1)],
            'framework_confidence': framework_conf,
            'style_embedding': style_embedding,
            'url': url
        }
    
    def predict_from_file(self, website_file: Path) -> Dict:
        """ä»JSONLæ–‡ä»¶é¢„æµ‹"""
        with open(website_file, 'r', encoding='utf-8') as f:
            data = json.loads(f.readline())
        
        # æå–æ•°æ®
        if 'pages' in data:
            # æ–°æ ¼å¼ï¼ˆå¤šé¡µé¢ï¼‰
            main_page = data['pages']['main']
            html = main_page['html']
            css = '\n'.join([f['content'] for f in main_page.get('css_files', [])])
            js = '\n'.join([f['content'] for f in main_page.get('js_files', [])])
        else:
            # æ—§æ ¼å¼ï¼ˆå•é¡µé¢ï¼‰
            html = data.get('html', '')
            css = '\n'.join([f['content'] for f in data.get('css_files', [])])
            js = '\n'.join([f['content'] for f in data.get('js_files', [])])
        
        url = data['url']
        
        return self.predict(html, css, js, url)
    
    def find_similar_websites(self, target_embedding, embeddings_db: List[Dict], top_k: int = 5) -> List[Dict]:
        """æ‰¾åˆ°ç›¸ä¼¼çš„ç½‘ç«™"""
        similarities = []
        target_tensor = torch.tensor(target_embedding)
        
        for item in embeddings_db:
            emb_tensor = torch.tensor(item['embedding'])
            similarity = F.cosine_similarity(target_tensor, emb_tensor, dim=0).item()
            
            similarities.append({
                'url': item['url'],
                'category': item['category'],
                'similarity': similarity
            })
        
        # æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]
    
    def batch_inference(self, data_file: Path, output_file: Path, max_samples: int = None):
        """æ‰¹é‡æ¨ç†"""
        logger.info(f"ğŸ“Š æ‰¹é‡æ¨ç†: {data_file}")
        
        results = []
        embeddings_db = []
        
        with open(data_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                
                try:
                    data = json.loads(line)
                    
                    # æå–æ•°æ®
                    if 'pages' in data:
                        main_page = data['pages']['main']
                        html = main_page['html']
                        css = '\n'.join([f['content'] for f in main_page.get('css_files', [])])
                        js = '\n'.join([f['content'] for f in main_page.get('js_files', [])])
                    else:
                        html = data.get('html', '')
                        css = '\n'.join([f['content'] for f in data.get('css_files', [])])
                        js = '\n'.join([f['content'] for f in data.get('js_files', [])])
                    
                    url = data['url']
                    
                    # é¢„æµ‹
                    result = self.predict(html, css, js, url)
                    results.append(result)
                    
                    # ä¿å­˜åµŒå…¥
                    embeddings_db.append({
                        'url': url,
                        'category': result['category'],
                        'embedding': result['style_embedding'].tolist()
                    })
                    
                    if (i + 1) % 100 == 0:
                        logger.info(f"âœ… å·²å¤„ç† {i+1} ä¸ªç½‘ç«™")
                
                except Exception as e:
                    logger.error(f"âŒ é”™è¯¯ (line {i+1}): {e}")
                    continue
        
        # ä¿å­˜ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'results': results,
                'embeddings_db': embeddings_db,
                'total': len(results)
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
        
        # ç»Ÿè®¡
        category_counts = {}
        framework_counts = {}
        
        for r in results:
            cat = r['category']
            fw = r['framework']
            category_counts[cat] = category_counts.get(cat, 0) + 1
            framework_counts[fw] = framework_counts.get(fw, 0) + 1
        
        logger.info("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            logger.info(f"  {cat}: {count}")
        
        logger.info("\nğŸ¨ æ¡†æ¶ç»Ÿè®¡:")
        for fw, count in sorted(framework_counts.items(), key=lambda x: -x[1]):
            logger.info(f"  {fw}: {count}")
        
        return results, embeddings_db


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ç½‘ç«™æ¨ç†ä¸ç”Ÿæˆ")
    parser.add_argument("--model", type=Path, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--mode", choices=['single', 'batch'], default='single', help="æ¨ç†æ¨¡å¼")
    parser.add_argument("--input", type=Path, help="è¾“å…¥æ–‡ä»¶")
    parser.add_argument("--output", type=Path, help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--url", type=str, help="ç½‘ç«™URLï¼ˆå•ä¸ªæ¨ç†ï¼‰")
    parser.add_argument("--html", type=str, help="HTMLå†…å®¹")
    parser.add_argument("--css", type=str, help="CSSå†…å®¹")
    parser.add_argument("--js", type=str, help="JSå†…å®¹")
    parser.add_argument("--max-samples", type=int, help="æœ€å¤§æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = WebsiteInference(args.model)
    
    if args.mode == 'single':
        if args.input:
            # ä»æ–‡ä»¶æ¨ç†
            result = engine.predict_from_file(args.input)
        else:
            # ä»å‚æ•°æ¨ç†
            result = engine.predict(
                html=args.html or "",
                css=args.css or "",
                js=args.js or "",
                url=args.url or "http://example.com"
            )
        
        print("\n" + "="*60)
        print("ğŸ¯ æ¨ç†ç»“æœ:")
        print("="*60)
        print(f"\nğŸ“ URL: {result['url']}")
        print(f"\nğŸ·ï¸  åˆ†ç±»: {result['category']} ({result['category_confidence']*100:.1f}%)")
        print(f"\nğŸ¨ æ¡†æ¶: {result['framework']} ({result['framework_confidence']*100:.1f}%)")
        print(f"\nğŸ“Š Top-3 åˆ†ç±»:")
        for item in result['top3_categories']:
            print(f"  - {item['category']}: {item['confidence']*100:.1f}%")
        print("\n" + "="*60)
        
    elif args.mode == 'batch':
        if not args.input or not args.output:
            parser.error("æ‰¹é‡æ¨¡å¼éœ€è¦ --input å’Œ --output å‚æ•°")
        
        results, embeddings_db = engine.batch_inference(
            args.input,
            args.output,
            max_samples=args.max_samples
        )
        
        print(f"\nâœ… æ‰¹é‡æ¨ç†å®Œæˆ: {len(results)} ä¸ªç½‘ç«™")


if __name__ == "__main__":
    main()
