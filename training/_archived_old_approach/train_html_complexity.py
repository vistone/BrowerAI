#!/usr/bin/env python3
"""
BrowerAI - HTML å¤æ‚åº¦é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬

ä»åé¦ˆæ•°æ®ä¸­å­¦ä¹ é¢„æµ‹ HTML æ–‡æ¡£çš„å¤æ‚åº¦ï¼ˆ0.0-1.0ï¼‰
ä½¿ç”¨ PyTorch + ONNX Export

ç”¨æ³•:
    python train_html_complexity.py --data ../data/feedback_*.json --epochs 100
"""

import json
import glob
import os
import argparse
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np


class FeedbackDataset(Dataset):
    """åé¦ˆæ•°æ®é›†"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels).reshape(-1, 1)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


class HtmlComplexityModel(nn.Module):
    """HTML å¤æ‚åº¦é¢„æµ‹æ¨¡å‹
    
    è¾“å…¥: 100 ç»´ç‰¹å¾å‘é‡
    è¾“å‡º: å¤æ‚åº¦è¯„åˆ† 0.0-1.0
    """
    
    def __init__(self, input_size=100, hidden_sizes=[128, 64, 32]):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, 1))
        layers.append(nn.Sigmoid())
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


def load_feedback_data(pattern: str) -> List[dict]:
    """åŠ è½½æ‰€æœ‰åŒ¹é…çš„åé¦ˆæ–‡ä»¶"""
    all_data = []
    files = glob.glob(pattern)
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(files)} ä¸ªåé¦ˆæ–‡ä»¶")
    
    for file in sorted(files):
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
                print(f"   âœ“ {Path(file).name}: {len(data)} ä¸ªäº‹ä»¶")
        except Exception as e:
            print(f"   âœ— {Path(file).name}: {e}")
    
    return all_data


def extract_html_features(event: dict) -> Tuple[List[float], float]:
    """ä» HTML è§£æäº‹ä»¶ä¸­æå–ç‰¹å¾
    
    ç‰¹å¾åŒ…æ‹¬:
    - å¤æ‚åº¦ï¼ˆå½“å‰å€¼ï¼Œç”¨äºè®­ç»ƒï¼‰
    - æˆåŠŸæ ‡å¿—
    - AI ä½¿ç”¨æ ‡å¿—
    - æ—¶é—´æˆ³ï¼ˆå°æ—¶ã€æ˜ŸæœŸå‡ ç­‰æ—¶é—´ç‰¹å¾ï¼‰
    - é”™è¯¯ä¿¡æ¯ï¼ˆæ˜¯å¦æœ‰é”™è¯¯ï¼‰
    
    å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦ä»çœŸå® HTML æå–æ›´å¤šç‰¹å¾ï¼š
    - æ ‡ç­¾æ•°é‡ã€åµŒå¥—æ·±åº¦ã€æ–‡æœ¬é•¿åº¦
    - è¡¨æ ¼/è¡¨å•/å¤šåª’ä½“å…ƒç´ æ•°é‡
    - å±æ€§æ•°é‡ã€class/id ä½¿ç”¨æƒ…å†µ
    - è¯­ä¹‰æ ‡ç­¾ä½¿ç”¨æƒ…å†µç­‰
    """
    
    features = []
    
    # åŸºç¡€ç‰¹å¾
    features.append(1.0 if event.get('success', True) else 0.0)
    features.append(1.0 if event.get('ai_used', False) else 0.0)
    features.append(1.0 if event.get('error') else 0.0)
    
    # å½“å‰å¤æ‚åº¦ï¼ˆç”¨äºåŠç›‘ç£å­¦ä¹ ï¼‰
    current_complexity = event.get('complexity', 0.5)
    features.append(current_complexity)
    
    # æ—¶é—´ç‰¹å¾ï¼ˆä» timestamp æå–ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æå–æ›´å¤š
    features.append(0.5)  # å°æ—¶å½’ä¸€åŒ–
    features.append(0.5)  # æ˜ŸæœŸå½’ä¸€åŒ–
    
    # å¡«å……åˆ° 100 ç»´ï¼ˆå®é™…åº”ç”¨ä¸­ç”¨çœŸå®ç‰¹å¾æ›¿æ¢ï¼‰
    while len(features) < 100:
        features.append(0.0)
    
    # æ ‡ç­¾æ˜¯å¤æ‚åº¦å€¼
    label = current_complexity
    
    return features[:100], label


def prepare_dataset(feedback_events: List[dict]) -> Tuple[np.ndarray, np.ndarray]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
    
    features_list = []
    labels_list = []
    
    for event in feedback_events:
        if event.get('type') == 'html_parsing':
            try:
                features, label = extract_html_features(event)
                features_list.append(features)
                labels_list.append(label)
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡äº‹ä»¶: {e}")
                continue
    
    features = np.array(features_list, dtype=np.float32)
    labels = np.array(labels_list, dtype=np.float32)
    
    return features, labels


def train_model(
    train_loader: DataLoader,
    val_loader: DataLoader,
    model: nn.Module,
    epochs: int,
    lr: float,
    device: str
) -> nn.Module:
    """è®­ç»ƒæ¨¡å‹"""
    
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    # Note: PyTorch 2.9.1's ReduceLROnPlateau does not support `verbose`
    # in some builds; keep a quiet scheduler and log manually when LR steps.
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )
    
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 20
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        if new_lr != old_lr:
            print(f"   â†˜ï¸  å­¦ä¹ ç‡é™ä½: {old_lr:.6f} -> {new_lr:.6f}")
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch [{epoch+1}/{epochs}] "
                  f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # æ—©åœ
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), '../models/html_complexity_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= max_patience:
                print(f"\nâ¸ï¸  æ—©åœäº Epoch {epoch+1}ï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('../models/html_complexity_best.pth'))
    return model


def export_onnx(model: nn.Module, output_path: str):
    """å¯¼å‡ºä¸º ONNX æ ¼å¼"""
    
    model.eval()
    dummy_input = torch.randn(1, 100)
    
    # Export to temporary location first
    import tempfile
    import shutil
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_model_path = os.path.join(tmpdir, "model.onnx")
        
        torch.onnx.export(
            model,
            dummy_input,
            tmp_model_path,
            input_names=['features'],
            output_names=['complexity'],
            dynamic_axes={
                'features': {0: 'batch_size'},
                'complexity': {0: 'batch_size'}
            },
            # Use latest stable opset to avoid version-conversion failures (torch 2.9 emits opset 18)
            opset_version=18,
            do_constant_folding=True,
            export_params=True,
        )
        
        # Load and re-save to embed external data
        import onnx
        onnx_model = onnx.load(tmp_model_path, load_external_data=True)
        onnx.save_model(onnx_model, output_path, save_as_external_data=False)
    
    
    print(f"âœ… ONNX æ¨¡å‹å·²å¯¼å‡ºåˆ°: {output_path}")
    
    # éªŒè¯ ONNX æ¨¡å‹
    try:
        import onnx
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        print("âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"   æ¨¡å‹ç‰ˆæœ¬: {onnx_model.opset_import[0].version}")
        print(f"   è¾“å…¥: {onnx_model.graph.input[0].name}")
        print(f"   è¾“å‡º: {onnx_model.graph.output[0].name}")
    except ImportError:
        print("âš ï¸  æœªå®‰è£… onnxï¼Œè·³è¿‡éªŒè¯ï¼ˆå»ºè®®: pip install onnxï¼‰")
    except Exception as e:
        print(f"âš ï¸  ONNX éªŒè¯å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒ HTML å¤æ‚åº¦é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--data', type=str, default='../data/feedback_*.json',
                        help='åé¦ˆæ•°æ®æ–‡ä»¶æ¨¡å¼')
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--val-split', type=float, default=0.2,
                        help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--output', type=str, default='../models/html_complexity_v1.onnx',
                        help='ONNX è¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½åé¦ˆæ•°æ®...")
    feedback_data = load_feedback_data(args.data)
    print(f"âœ… æ€»å…±åŠ è½½ {len(feedback_data)} ä¸ªåé¦ˆäº‹ä»¶")
    
    # å‡†å¤‡æ•°æ®é›†
    print("\nğŸ” æå–ç‰¹å¾...")
    features, labels = prepare_dataset(feedback_data)
    print(f"âœ… ç‰¹å¾çŸ©é˜µ: {features.shape}")
    print(f"   æ ‡ç­¾èŒƒå›´: [{labels.min():.2f}, {labels.max():.2f}]")
    
    if len(features) < 10:
        print("\nâŒ æ•°æ®é‡å¤ªå°‘ï¼ˆ< 10ï¼‰ï¼Œæ— æ³•è®­ç»ƒï¼")
        print("   å»ºè®®: å…ˆè¿è¡Œ 'cargo run -- --learn' æ”¶é›†æ›´å¤šæ•°æ®")
        return
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(features) * args.val_split)
    train_size = len(features) - val_size
    
    indices = np.random.permutation(len(features))
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    train_dataset = FeedbackDataset(features[train_indices], labels[train_indices])
    val_dataset = FeedbackDataset(features[val_indices], labels[val_indices])
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = HtmlComplexityModel()
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ… æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # è®­ç»ƒ
    print(f"\nğŸ“ å¼€å§‹è®­ç»ƒï¼ˆ{args.epochs} è½®ï¼‰...")
    model = train_model(train_loader, val_loader, model, args.epochs, args.lr, device)
    
    # å¯¼å‡º ONNX
    print("\nğŸ’¾ å¯¼å‡º ONNX æ¨¡å‹...")
    Path(args.output).parent.mkdir(parents=True, exist_ok=True)
    export_onnx(model, args.output)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"  1. å¤åˆ¶æ¨¡å‹: cp {args.output} ../../models/local/")
    print("  2. æ›´æ–°é…ç½®: vim ../../models/model_config.toml")
    print("     æ·»åŠ :")
    print("     [[models]]")
    print("     name = \"html_complexity_v1\"")
    print("     model_type = \"HtmlParser\"")
    print("     path = \"html_complexity_v1.onnx\"")
    print("     version = \"1.0.0\"")
    print("     enabled = true")
    print("  3. é‡æ–°ç¼–è¯‘: cd ../.. && cargo build --release --features ai")
    print("  4. æµ‹è¯•æ•ˆæœ: cargo run -- --ai-report")


if __name__ == '__main__':
    main()
