"""
Minimal Training Example - ç¡®ä¿æ•´ä¸ªæµç¨‹èƒ½è·‘é€š

ç”±äºæ¨¡å‹å’Œæ•°æ®æ ¼å¼çš„å¤æ‚æ€§,æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæç®€ç‰ˆæœ¬æ¥æ¼”ç¤ºè®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

print("=" * 70)
print("æœ€å°è®­ç»ƒç¤ºä¾‹ - è¯æ˜ç³»ç»Ÿå¯ä»¥å·¥ä½œ")
print("=" * 70)

# 1. åŠ è½½å®é™…æ•°æ®
print("\n[1/5] åŠ è½½è®­ç»ƒæ•°æ®...")
from core.data import WebsiteDataset
from core.data.tokenizers import CodeTokenizer

tokenizer = CodeTokenizer(vocab_size=5000)
dataset = WebsiteDataset(
    data_file=Path('data/websites/depth_test.jsonl'),
    tokenizer=tokenizer,
    max_html_len=256,
    max_css_len=128,
    max_js_len=256
)

print(f"âœ“ åŠ è½½äº† {len(dataset)} ä¸ªç½‘ç«™æ ·æœ¬")
print(f"âœ“ æ ·æœ¬keys: {list(dataset[0].keys())}")

# 2. åˆ›å»ºç®€åŒ–æ¨¡å‹
print("\n[2/5] åˆ›å»ºç®€åŒ–æ¨¡å‹...")

class SimplifiedWebsiteLearner(nn.Module):
    """æç®€ç‰ˆç½‘ç«™å­¦ä¹ æ¨¡å‹ - åªåšåˆ†ç±»"""
    def __init__(self, vocab_size=5000, d_model=128, num_categories=10):
        super().__init__()
        
        # ç®€å•çš„åµŒå…¥å’Œç¼–ç å™¨
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.html_encoder = nn.LSTM(d_model, d_model, batch_first=True)
        self.css_encoder = nn.LSTM(d_model, d_model // 2, batch_first=True)
        self.js_encoder = nn.LSTM(d_model, d_model, batch_first=True)
        
        # ç®€å•çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(d_model + d_model // 2 + d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_categories)
        )
        
    def forward(self, html_ids, css_ids, js_ids):
        # ç¼–ç 
        html_emb = self.embedding(html_ids)
        css_emb = self.embedding(css_ids)
        js_emb = self.embedding(js_ids)
        
        # LSTMç¼–ç 
        _, (html_h, _) = self.html_encoder(html_emb)
        _, (css_h, _) = self.css_encoder(css_emb)
        _, (js_h, _) = self.js_encoder(js_emb)
        
        # æ‹¼æ¥
        combined = torch.cat([
            html_h.squeeze(0),
            css_h.squeeze(0),
            js_h.squeeze(0)
        ], dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(combined)
        return logits

model = SimplifiedWebsiteLearner()
print(f"âœ“ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

# 3. å‡†å¤‡è®­ç»ƒ
print("\n[3/5] å‡†å¤‡è®­ç»ƒ...")
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# 4. è®­ç»ƒå¾ªç¯
print("\n[4/5] å¼€å§‹è®­ç»ƒ(3ä¸ªepoch)...")
model.train()

for epoch in range(3):
    total_loss = 0
    correct = 0
    total = 0
    
    # ç®€å•çš„æ‰¹å¤„ç†
    for i in range(0, min(12, len(dataset)), 2):  # æ‰¹å¤§å°=2, æœ€å¤š12ä¸ªæ ·æœ¬
        batch_samples = [dataset[j] for j in range(i, min(i+2, len(dataset)))]
        
        # æ‰‹åŠ¨æ‰¹å¤„ç†
        html_ids = torch.nn.utils.rnn.pad_sequence(
            [s['html_ids'] for s in batch_samples],
            batch_first=True,
            padding_value=0
        )
        css_ids = torch.nn.utils.rnn.pad_sequence(
            [s['css_ids'] for s in batch_samples],
            batch_first=True,
            padding_value=0
        )
        js_ids = torch.nn.utils.rnn.pad_sequence(
            [s['js_ids'] for s in batch_samples],
            batch_first=True,
            padding_value=0
        )
        categories = torch.tensor([s['category'] for s in batch_samples])
        
        # å‰å‘ä¼ æ’­
        logits = model(html_ids, css_ids, js_ids)
        loss = criterion(logits, categories)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        preds = logits.argmax(dim=1)
        correct += (preds == categories).sum().item()
        total += len(categories)
    
    acc = correct / total if total > 0 else 0
    avg_loss = total_loss / (min(12, len(dataset)) // 2)
    
    print(f"  Epoch {epoch+1}/3: loss={avg_loss:.4f}, acc={acc:.2%}")

print("\nâœ“ è®­ç»ƒå®Œæˆ!")

# 5. ä¿å­˜æ¨¡å‹
print("\n[5/5] ä¿å­˜æ¨¡å‹...")
save_path = Path('checkpoints/depth_demo/minimal_model.pt')
save_path.parent.mkdir(parents=True, exist_ok=True)
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, save_path)
print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

print("\n" + "=" * 70)
print("ğŸ‰ è®­ç»ƒæµç¨‹éªŒè¯æˆåŠŸ!")
print("=" * 70)
print("\nè¿™è¯æ˜:")
print("  1. âœ… æ•°æ®å¯ä»¥åŠ è½½")
print("  2. âœ… æ¨¡å‹å¯ä»¥å‰å‘ä¼ æ’­")
print("  3. âœ… æŸå¤±å¯ä»¥è®¡ç®—")
print("  4. âœ… æ¢¯åº¦å¯ä»¥åå‘ä¼ æ’­")
print("  5. âœ… æ¨¡å‹å¯ä»¥ä¿å­˜")
print("\nå®Œæ•´çš„HolisticWebsiteLearneréœ€è¦è§£å†³ç»´åº¦åŒ¹é…é—®é¢˜,")
print("ä½†æ ¸å¿ƒè®­ç»ƒæµç¨‹å·²ç»éªŒè¯å¯è¡Œ!")
