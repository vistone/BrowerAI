# å¤§è§„æ¨¡ç½‘ç«™å­¦ä¹  - å®Œæ•´å·¥ä½œæµ

æœ¬æŒ‡å—æè¿°å¦‚ä½•çˆ¬å–1000+ç½‘ç«™ã€è®­ç»ƒæ¨¡å‹ã€å¹¶è¿›è¡Œæ¨ç†ç”Ÿæˆã€‚

## ğŸ¯ ç›®æ ‡

1. çˆ¬å–1000+ä¸ªçœŸå®ç½‘ç«™æ•°æ®
2. è®­ç»ƒå®Œæ•´çš„ç½‘ç«™ç†è§£æ¨¡å‹
3. ä¿å­˜æ¨¡å‹æƒé‡åˆ°æœ¬åœ°
4. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†å’Œç”Ÿæˆ

---

## ğŸ“‹ å‡†å¤‡å·¥ä½œ

### 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶

```bash
cd /workspaces/BrowerAI/training

# æŸ¥çœ‹URLåˆ—è¡¨
head data/large_urls.txt
wc -l data/large_urls.txt  # åº”æ˜¾ç¤ºçº¦1000è¡Œ
```

### 2. å®‰è£…ä¾èµ–ï¼ˆå¦‚éœ€è¦ï¼‰

```bash
pip install tqdm aiohttp beautifulsoup4 torch
```

---

## ğŸ•·ï¸ ç¬¬ä¸€æ­¥: å¤§è§„æ¨¡çˆ¬å–

### æ–¹æ¡ˆA: å®Œæ•´çˆ¬å–ï¼ˆæ¨èï¼‰

çˆ¬å–1000ä¸ªç½‘ç«™ï¼Œæ¯ä¸ªç½‘ç«™2å±‚æ·±åº¦ï¼Œæ¯ç«™æœ€å¤š5ä¸ªé¡µé¢ï¼š

```bash
python scripts/batch_crawl_websites.py \
  --urls-file data/large_urls.txt \
  --output-dir data/websites/large_scale \
  --batch-size 50 \
  --depth 2 \
  --max-pages 5 \
  --output data/websites/large_train.jsonl
```

**é¢„è®¡ç»“æœ**:
- ç½‘ç«™æ•°é‡: ~1000 ä¸ª
- é¡µé¢æ€»æ•°: ~3000-4000 é¡µ
- æ—¶é—´æ¶ˆè€—: 6-10 å°æ—¶ï¼ˆå–å†³äºç½‘ç»œï¼‰
- å­˜å‚¨ç©ºé—´: ~2-5 GB

**ç‰¹æ€§**:
- âœ… æ–­ç‚¹ç»­ä¼ ï¼ˆä¸­æ–­åå¯æ¢å¤ï¼‰
- âœ… æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯50ä¸ªç½‘ç«™ä¸€æ‰¹ï¼‰
- âœ… é”™è¯¯é‡è¯•
- âœ… è¿›åº¦æ—¥å¿—ä¿å­˜

### æ–¹æ¡ˆB: å¿«é€Ÿæµ‹è¯•ï¼ˆ100ä¸ªç½‘ç«™ï¼‰

å¦‚æœåªæƒ³å¿«é€Ÿæµ‹è¯•ï¼š

```bash
# åˆ›å»ºæµ‹è¯•URLåˆ—è¡¨
head -100 data/large_urls.txt > data/test_100_urls.txt

# çˆ¬å–100ä¸ªç½‘ç«™
python scripts/batch_crawl_websites.py \
  --urls-file data/test_100_urls.txt \
  --output-dir data/websites/test_100 \
  --batch-size 20 \
  --depth 2 \
  --max-pages 5 \
  --output data/websites/test_100_train.jsonl
```

**é¢„è®¡ç»“æœ**:
- ç½‘ç«™æ•°é‡: ~100 ä¸ª
- é¡µé¢æ€»æ•°: ~300-400 é¡µ
- æ—¶é—´æ¶ˆè€—: 30-60 åˆ†é’Ÿ
- å­˜å‚¨ç©ºé—´: ~200-500 MB

### æ–¹æ¡ˆC: æ¢å¤ä¸­æ–­çš„çˆ¬å–

å¦‚æœçˆ¬å–ä¸­æ–­ï¼Œå¯ä»¥ç»§ç»­ï¼š

```bash
# ç³»ç»Ÿä¼šè‡ªåŠ¨ä» data/websites/large_scale/crawl_progress.json æ¢å¤
python scripts/batch_crawl_websites.py \
  --urls-file data/large_urls.txt \
  --output-dir data/websites/large_scale \
  --batch-size 50 \
  --depth 2 \
  --max-pages 5 \
  --output data/websites/large_train.jsonl
```

### æ–¹æ¡ˆD: åªåˆå¹¶å·²æœ‰æ‰¹æ¬¡

å¦‚æœå·²ç»çˆ¬å–äº†éƒ¨åˆ†æ‰¹æ¬¡ï¼Œåªæƒ³åˆå¹¶ï¼š

```bash
python scripts/batch_crawl_websites.py \
  --output-dir data/websites/large_scale \
  --output data/websites/large_train.jsonl \
  --merge
```

---

## ğŸ” æ£€æŸ¥çˆ¬å–ç»“æœ

```bash
# ç»Ÿè®¡ç½‘ç«™æ•°é‡
wc -l data/websites/large_train.jsonl

# æŸ¥çœ‹ç¬¬ä¸€ä¸ªç½‘ç«™
head -1 data/websites/large_train.jsonl | python -m json.tool | head -50

# ç»Ÿè®¡è¯¦ç»†ä¿¡æ¯
cat data/websites/large_train.jsonl | python -c "
import json
import sys

total_sites = 0
total_pages = 0
frameworks = {}

for line in sys.stdin:
    data = json.loads(line)
    total_sites += 1
    total_pages += data.get('depth', 1)
    fw = data.get('metadata', {}).get('framework', 'Unknown')
    frameworks[fw] = frameworks.get(fw, 0) + 1

print(f'ç½‘ç«™æ€»æ•°: {total_sites}')
print(f'é¡µé¢æ€»æ•°: {total_pages}')
print(f'å¹³å‡æ·±åº¦: {total_pages/total_sites:.1f}')
print(f'\næ¡†æ¶åˆ†å¸ƒ:')
for fw, count in sorted(frameworks.items(), key=lambda x: -x[1]):
    print(f'  {fw}: {count}')
"
```

---

## ğŸ¤– ç¬¬äºŒæ­¥: è®­ç»ƒæ¨¡å‹

### å®Œæ•´è®­ç»ƒï¼ˆæ¨èï¼‰

ä½¿ç”¨æ‰€æœ‰æ•°æ®è®­ç»ƒ50ä¸ªepochï¼š

```bash
python scripts/train_large_scale.py \
  --data-file data/websites/large_train.jsonl \
  --checkpoint-dir checkpoints/large_scale \
  --epochs 50 \
  --batch-size 8 \
  --learning-rate 1e-4
```

**è®­ç»ƒé…ç½®**:
- Epochæ•°: 50
- Batch size: 8
- å­¦ä¹ ç‡: 0.0001
- ä¼˜åŒ–å™¨: AdamW
- è°ƒåº¦å™¨: CosineAnnealingLR

**é¢„è®¡æ—¶é—´**:
- CPUè®­ç»ƒ: 20-40 å°æ—¶
- GPUè®­ç»ƒ: 2-5 å°æ—¶

**æ£€æŸ¥ç‚¹ä¿å­˜**:
- `checkpoints/large_scale/latest_checkpoint.pt` - æœ€æ–°æ£€æŸ¥ç‚¹
- `checkpoints/large_scale/best_model.pt` - æœ€ä½³æ¨¡å‹
- `checkpoints/large_scale/checkpoint_epoch_N.pt` - å®šæœŸä¿å­˜
- `checkpoints/large_scale/training_history.json` - è®­ç»ƒå†å²
- `checkpoints/large_scale/website_learner.onnx` - ONNXæ¨¡å‹

### å¿«é€Ÿè®­ç»ƒï¼ˆæµ‹è¯•ï¼‰

ä½¿ç”¨è¾ƒå°‘çš„epochå¿«é€Ÿæµ‹è¯•ï¼š

```bash
python scripts/train_large_scale.py \
  --data-file data/websites/test_100_train.jsonl \
  --checkpoint-dir checkpoints/test_100 \
  --epochs 10 \
  --batch-size 4 \
  --learning-rate 1e-4
```

### æ¢å¤è®­ç»ƒ

å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œæ·»åŠ  `--resume` ä»æ£€æŸ¥ç‚¹ç»§ç»­ï¼š

```bash
python scripts/train_large_scale.py \
  --data-file data/websites/large_train.jsonl \
  --checkpoint-dir checkpoints/large_scale \
  --epochs 50 \
  --batch-size 8 \
  --learning-rate 1e-4 \
  --resume
```

---

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹
tail -f checkpoints/large_scale/*.log

# æŸ¥çœ‹è®­ç»ƒå†å²
cat checkpoints/large_scale/training_history.json | python -m json.tool
```

### å¯è§†åŒ–è®­ç»ƒæ›²çº¿

```python
import json
import matplotlib.pyplot as plt

with open('checkpoints/large_scale/training_history.json') as f:
    history = json.load(f)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training Loss')

plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Acc')
plt.plot(history['val_acc'], label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.title('Training Accuracy')

plt.tight_layout()
plt.savefig('training_curves.png')
```

---

## ğŸ¯ ç¬¬ä¸‰æ­¥: æ¨ç†ä¸ç”Ÿæˆ

### å•ä¸ªç½‘ç«™æ¨ç†

```bash
python scripts/inference_website.py \
  --model checkpoints/large_scale/best_model.pt \
  --mode single \
  --url "https://example.com" \
  --html "<html>...</html>" \
  --css "body {...}" \
  --js "console.log('hello')"
```

### ä»æ–‡ä»¶æ¨ç†

```bash
python scripts/inference_website.py \
  --model checkpoints/large_scale/best_model.pt \
  --mode single \
  --input data/websites/depth_test.jsonl
```

### æ‰¹é‡æ¨ç†

å¯¹æ‰€æœ‰ç½‘ç«™è¿›è¡Œæ¨ç†ï¼š

```bash
python scripts/inference_website.py \
  --model checkpoints/large_scale/best_model.pt \
  --mode batch \
  --input data/websites/large_train.jsonl \
  --output results/inference_results.json \
  --max-samples 1000
```

**è¾“å‡ºæ–‡ä»¶åŒ…å«**:
- æ¯ä¸ªç½‘ç«™çš„åˆ†ç±»é¢„æµ‹
- æ¡†æ¶è¯†åˆ«ç»“æœ
- é£æ ¼åµŒå…¥å‘é‡
- ç›¸ä¼¼åº¦è®¡ç®—æ•°æ®

### æŸ¥çœ‹æ¨ç†ç»“æœ

```bash
# æŸ¥çœ‹ç»“æœ
cat results/inference_results.json | python -m json.tool | head -100

# ç»Ÿè®¡åˆ†ç±»å‡†ç¡®ç‡
cat results/inference_results.json | python -c "
import json
import sys

data = json.load(sys.stdin)
results = data['results']

print(f'æ€»æ¨ç†æ•°é‡: {len(results)}')
print(f'\nåˆ†ç±»åˆ†å¸ƒ:')
categories = {}
for r in results:
    cat = r['category']
    conf = r['category_confidence']
    categories[cat] = categories.get(cat, 0) + 1

for cat, count in sorted(categories.items(), key=lambda x: -x[1]):
    print(f'  {cat}: {count}')
"
```

---

## ğŸ“¦ æ¨¡å‹æ–‡ä»¶è¯´æ˜

è®­ç»ƒå®Œæˆåï¼Œ`checkpoints/large_scale/` åŒ…å«ï¼š

| æ–‡ä»¶ | å¤§å° | è¯´æ˜ |
|------|------|------|
| `best_model.pt` | ~200MB | æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±æœ€ä½ï¼‰ |
| `latest_checkpoint.pt` | ~200MB | æœ€æ–°æ£€æŸ¥ç‚¹ |
| `website_learner.onnx` | ~200MB | ONNXæ ¼å¼ï¼ˆç”¨äºéƒ¨ç½²ï¼‰ |
| `training_history.json` | ~10KB | è®­ç»ƒå†å² |
| `checkpoint_epoch_*.pt` | ~200MB | å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹ |

### ä½¿ç”¨ONNXæ¨¡å‹

```python
import onnxruntime as ort
import numpy as np

# åŠ è½½ONNXæ¨¡å‹
session = ort.InferenceSession('checkpoints/large_scale/website_learner.onnx')

# å‡†å¤‡è¾“å…¥
html_ids = np.random.randint(0, 10000, (1, 2048), dtype=np.int64)
css_ids = np.random.randint(0, 10000, (1, 1024), dtype=np.int64)
js_ids = np.random.randint(0, 10000, (1, 2048), dtype=np.int64)
url_features = np.random.randn(1, 128).astype(np.float32)

# æ¨ç†
outputs = session.run(None, {
    'html_ids': html_ids,
    'css_ids': css_ids,
    'js_ids': js_ids,
    'url_features': url_features
})

print('Category logits:', outputs[0])
print('Framework logits:', outputs[1])
print('Style embedding:', outputs[2])
```

---

## ğŸ”„ å®Œæ•´å·¥ä½œæµç¤ºä¾‹

### ç¤ºä¾‹1: å®Œæ•´æµç¨‹ï¼ˆ1000ç½‘ç«™ï¼‰

```bash
#!/bin/bash
# å®Œæ•´å·¥ä½œæµ

echo "==== ç¬¬1æ­¥: çˆ¬å–1000ä¸ªç½‘ç«™ ===="
python scripts/batch_crawl_websites.py \
  --urls-file data/large_urls.txt \
  --output-dir data/websites/large_scale \
  --batch-size 50 \
  --depth 2 \
  --max-pages 5 \
  --output data/websites/large_train.jsonl

echo "==== ç¬¬2æ­¥: è®­ç»ƒæ¨¡å‹ ===="
python scripts/train_large_scale.py \
  --data-file data/websites/large_train.jsonl \
  --checkpoint-dir checkpoints/large_scale \
  --epochs 50 \
  --batch-size 8 \
  --learning-rate 1e-4

echo "==== ç¬¬3æ­¥: æ‰¹é‡æ¨ç† ===="
python scripts/inference_website.py \
  --model checkpoints/large_scale/best_model.pt \
  --mode batch \
  --input data/websites/large_train.jsonl \
  --output results/inference_results.json

echo "==== å®Œæˆ! ===="
```

### ç¤ºä¾‹2: å¿«é€Ÿæµ‹è¯•æµç¨‹ï¼ˆ100ç½‘ç«™ï¼‰

```bash
#!/bin/bash
# å¿«é€Ÿæµ‹è¯•å·¥ä½œæµ

# å‡†å¤‡æµ‹è¯•æ•°æ®
head -100 data/large_urls.txt > data/test_100_urls.txt

# çˆ¬å–
python scripts/batch_crawl_websites.py \
  --urls-file data/test_100_urls.txt \
  --output-dir data/websites/test_100 \
  --batch-size 20 \
  --depth 2 \
  --max-pages 5 \
  --output data/websites/test_100_train.jsonl

# è®­ç»ƒ
python scripts/train_large_scale.py \
  --data-file data/websites/test_100_train.jsonl \
  --checkpoint-dir checkpoints/test_100 \
  --epochs 10 \
  --batch-size 4

# æ¨ç†
python scripts/inference_website.py \
  --model checkpoints/test_100/best_model.pt \
  --mode batch \
  --input data/websites/test_100_train.jsonl \
  --output results/test_100_results.json

echo "æµ‹è¯•å®Œæˆ!"
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### çˆ¬å–é˜¶æ®µ
1. **å°Šé‡robots.txt**: çˆ¬è™«ä¼šéµå®ˆç½‘ç«™çš„robotsåè®®
2. **é€Ÿç‡é™åˆ¶**: æ‰¹æ¬¡é—´æœ‰30ç§’ä¼‘æ¯ï¼Œé¿å…è¢«å°IP
3. **é”™è¯¯å¤„ç†**: å¤±è´¥çš„ç½‘ç«™ä¼šè¢«è®°å½•ä½†ä¸ä¼šä¸­æ–­æµç¨‹
4. **æ–­ç‚¹ç»­ä¼ **: ä½¿ç”¨ `crawl_progress.json` è·Ÿè¸ªè¿›åº¦

### è®­ç»ƒé˜¶æ®µ
1. **å†…å­˜éœ€æ±‚**: å»ºè®®è‡³å°‘16GB RAM
2. **GPUåŠ é€Ÿ**: æœ‰GPUä¼šå¿«å¾ˆå¤šï¼ˆ20x+ï¼‰
3. **æ£€æŸ¥ç‚¹ä¿å­˜**: æ¯5ä¸ªepochè‡ªåŠ¨ä¿å­˜
4. **è¿‡æ‹Ÿåˆ**: ç›‘æ§éªŒè¯é›†æŸå¤±ï¼Œå¿…è¦æ—¶æ—©åœ

### æ¨ç†é˜¶æ®µ
1. **æ‰¹é‡å¤„ç†**: å¤§è§„æ¨¡æ¨ç†æ—¶ä½¿ç”¨batchæ¨¡å¼
2. **ç»“æœç¼“å­˜**: æ¨ç†ç»“æœä¿å­˜ä¸ºJSONå¯é‡å¤ä½¿ç”¨
3. **ONNXéƒ¨ç½²**: ç”Ÿäº§ç¯å¢ƒæ¨èä½¿ç”¨ONNXæ ¼å¼

---

## ğŸ‰ é¢„æœŸç»“æœ

å®Œæˆæ•´ä¸ªæµç¨‹åï¼Œæ‚¨å°†æ‹¥æœ‰ï¼š

1. **æ•°æ®é›†**: 1000ä¸ªç½‘ç«™ï¼Œ~3000-4000ä¸ªé¡µé¢
2. **æ¨¡å‹**: è®­ç»ƒå¥½çš„ç½‘ç«™ç†è§£æ¨¡å‹ï¼ˆ~200MBï¼‰
3. **æ¨ç†ç»“æœ**: å®Œæ•´çš„ç½‘ç«™åˆ†ç±»å’Œåˆ†æ
4. **åµŒå…¥å‘é‡**: å¯ç”¨äºç›¸ä¼¼åº¦æœç´¢å’Œæ¨è

**æ¨¡å‹èƒ½åŠ›**:
- ğŸ·ï¸  ç½‘ç«™åˆ†ç±»ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
- ğŸ¨ æ¡†æ¶è¯†åˆ«ï¼ˆ8ç§ä¸»æµæ¡†æ¶ï¼‰
- ğŸ“Š é£æ ¼åˆ†æï¼ˆåµŒå…¥å‘é‡ï¼‰
- ğŸ” ç›¸ä¼¼ç½‘ç«™æ¨è

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DEPTH_QUICKREF.md](DEPTH_QUICKREF.md) - æ·±åº¦çˆ¬å–å¿«é€Ÿå‚è€ƒ
- [HOLISTIC_LEARNING_GUIDE.md](HOLISTIC_LEARNING_GUIDE.md) - æ•´ä½“å­¦ä¹ æŒ‡å—
- [README.md](README.md) - ä¸»æ–‡æ¡£

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**
