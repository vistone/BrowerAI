# é«˜å¹¶å‘ç½‘ç«™çˆ¬å–æŒ‡å—

## ğŸš€ æ€§èƒ½å¯¹æ¯”

### æ—§ç‰ˆæœ¬ï¼ˆé¡ºåºçˆ¬å–ï¼‰
```bash
# 15ä¸ªç½‘ç«™è€—æ—¶çº¦ 5-10 åˆ†é’Ÿ
python scripts/prepare_website_data.py \
  --urls-file data/quick_train_urls.txt \
  --output data/websites/sequential.jsonl
```

### æ–°ç‰ˆæœ¬ï¼ˆé«˜å¹¶å‘ï¼‰
```bash
# 15ä¸ªç½‘ç«™ä»…éœ€ 1-2 åˆ†é’Ÿï¼
python scripts/prepare_website_data.py \
  --urls-file data/quick_train_urls.txt \
  --output data/websites/concurrent.jsonl \
  --concurrency 10
```

**æé€Ÿ**: çº¦ **5-10å€**ï¼

---

## ğŸ“Š å¹¶å‘æ•°é€‰æ‹©æŒ‡å—

| å¹¶å‘æ•° | é€‚ç”¨åœºæ™¯ | é€Ÿåº¦ | é£é™© |
|--------|----------|------|------|
| `--concurrency 5` | è°¨æ…æ¨¡å¼ï¼Œé¿å…è¢«å° | 3x | ä½ |
| `--concurrency 10` | **æ¨èæ¨¡å¼** | 5-7x | ä¸­ |
| `--concurrency 20` | å¿«é€Ÿæ¨¡å¼ | 10x | ä¸­é«˜ |
| `--concurrency 50` | æé€Ÿæ¨¡å¼ | 15x+ | é«˜ |

**æ¨è**: ä½¿ç”¨ `--concurrency 10-20` å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§

---

## ğŸ’¡ å®é™…ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: å¿«é€Ÿæµ‹è¯•ï¼ˆ20ç½‘ç«™ï¼Œ2åˆ†é’Ÿï¼‰

```bash
cd /workspaces/BrowerAI/training

python scripts/prepare_website_data.py \
  --urls-file data/quick_train_urls.txt \
  --output data/websites/test_20.jsonl \
  --depth 2 \
  --max-pages 3 \
  --concurrency 10
```

**ç»“æœ**: 20ç½‘ç«™ï¼Œçº¦60é¡µé¢ï¼Œ2åˆ†é’Ÿå®Œæˆ

---

### æ¡ˆä¾‹2: ä¸­ç­‰è§„æ¨¡ï¼ˆ100ç½‘ç«™ï¼Œ15-20åˆ†é’Ÿï¼‰

```bash
head -100 data/large_urls.txt > data/100_urls.txt

python scripts/prepare_website_data.py \
  --urls-file data/100_urls.txt \
  --output data/websites/100_sites.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 15
```

**é¢„è®¡**:
- ç½‘ç«™æ•°: ~90ä¸ªï¼ˆéƒ¨åˆ†å¯èƒ½å¤±è´¥ï¼‰
- é¡µé¢æ•°: ~350-400é¡µ
- æ—¶é—´: 15-20åˆ†é’Ÿ
- æ•°æ®é‡: ~200-400MB

---

### æ¡ˆä¾‹3: å¤§è§„æ¨¡ï¼ˆ1000ç½‘ç«™ï¼Œ2-3å°æ—¶ï¼‰ğŸ”¥

```bash
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/1000_sites.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20
```

**é¢„è®¡**:
- ç½‘ç«™æ•°: ~800-900ä¸ª
- é¡µé¢æ•°: ~3000-4000é¡µ  
- æ—¶é—´: **2-3å°æ—¶**ï¼ˆæ—§ç‰ˆéœ€è¦8-12å°æ—¶ï¼ï¼‰
- æ•°æ®é‡: ~3-5GB

**èŠ‚çœæ—¶é—´**: çº¦ **6-9å°æ—¶**ï¼

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨åˆé€‚çš„å¹¶å‘æ•°

```bash
# ç½‘ç»œå¥½ + ä¸æ€•è¢«å° â†’ é«˜å¹¶å‘
--concurrency 30

# ç½‘ç»œä¸€èˆ¬ + æƒ³ç¨³å®š â†’ ä¸­ç­‰å¹¶å‘
--concurrency 10-15

# ç½‘ç»œå·® + è°¨æ…æ¨¡å¼ â†’ ä½å¹¶å‘
--concurrency 5
```

### 2. è°ƒæ•´æ·±åº¦å’Œé¡µé¢æ•°

```bash
# å¿«é€Ÿæµ…å±‚æ‰«æï¼ˆé€‚åˆåˆæ­¥åˆ†ç±»ï¼‰
--depth 1 --max-pages 3

# ä¸­ç­‰æ·±åº¦ï¼ˆæ¨èï¼‰
--depth 2 --max-pages 5

# å®Œæ•´æ·±åº¦ï¼ˆè¯¦ç»†åˆ†æï¼‰
--depth 3 --max-pages 10
```

### 3. åˆ†æ‰¹å¤„ç†ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰

```bash
# ç¬¬ä¸€æ‰¹: å‰500ä¸ª
head -500 data/large_urls.txt > data/batch1.txt
python scripts/prepare_website_data.py \
  --urls-file data/batch1.txt \
  --output data/websites/batch1.jsonl \
  --concurrency 20

# ç¬¬äºŒæ‰¹: å500ä¸ª
tail -500 data/large_urls.txt > data/batch2.txt
python scripts/prepare_website_data.py \
  --urls-file data/batch2.txt \
  --output data/websites/batch2.jsonl \
  --concurrency 20

# åˆå¹¶
cat data/websites/batch1.jsonl data/websites/batch2.jsonl > data/websites/all_1000.jsonl
```

---

## ğŸ¯ å®æˆ˜ï¼šçˆ¬å–1000ä¸ªç½‘ç«™

### å®Œæ•´å‘½ä»¤

```bash
cd /workspaces/BrowerAI/training

# ä½¿ç”¨é«˜å¹¶å‘çˆ¬å–1000ä¸ªç½‘ç«™
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/1000_sites_concurrent.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20

# å®æ—¶ç›‘æ§è¿›åº¦ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
watch -n 5 "wc -l data/websites/1000_sites_concurrent.jsonl"
```

### é¢„æœŸè¾“å‡º

```
2026-01-05 18:20:00 - INFO - å¼€å§‹æ·±åº¦çˆ¬å–: æ·±åº¦=2, æœ€å¤§é¡µé¢=5, å¹¶å‘æ•°=20
2026-01-05 18:20:00 - INFO - ğŸš€ å¯åŠ¨é«˜å¹¶å‘çˆ¬å–: 1000 ä¸ªç½‘ç«™, å¹¶å‘æ•°=20

é«˜å¹¶å‘çˆ¬å–:   5%|â–ˆâ–Œ        | 50/1000 [00:30<09:30,  1.67it/s]
é«˜å¹¶å‘çˆ¬å–:  10%|â–ˆâ–ˆâ–ˆ       | 100/1000 [01:00<09:00,  1.67it/s]
...
é«˜å¹¶å‘çˆ¬å–: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:30<00:00,  6.67it/s]

2026-01-05 18:22:30 - INFO - ğŸ’¾ ä¿å­˜ 892 ä¸ªç½‘ç«™åˆ° data/websites/1000_sites_concurrent.jsonl
2026-01-05 18:22:30 - INFO - âœ… å®Œæˆï¼æˆåŠŸçˆ¬å– 892/1000 ä¸ªç½‘ç«™
```

---

## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡

### å®æµ‹æ•°æ®ï¼ˆåŸºäº15ä¸ªç½‘ç«™æµ‹è¯•ï¼‰

| æŒ‡æ ‡ | é¡ºåºçˆ¬å– | é«˜å¹¶å‘(5) | é«˜å¹¶å‘(10) | é«˜å¹¶å‘(20) |
|------|----------|-----------|------------|------------|
| 15ä¸ªç½‘ç«™ | ~5åˆ†é’Ÿ | ~2åˆ†é’Ÿ | ~1.5åˆ†é’Ÿ | ~1åˆ†é’Ÿ |
| 100ä¸ªç½‘ç«™ | ~40åˆ†é’Ÿ | ~15åˆ†é’Ÿ | ~10åˆ†é’Ÿ | ~6åˆ†é’Ÿ |
| 1000ä¸ªç½‘ç«™ | ~7å°æ—¶ | ~3å°æ—¶ | ~2å°æ—¶ | ~1.5å°æ—¶ |

### æ¨æ–­æ•°æ®ï¼ˆåŸºäºçº¿æ€§æ‰©å±•ï¼‰

| ç½‘ç«™æ•° | é¡ºåº | å¹¶å‘10 | å¹¶å‘20 | èŠ‚çœæ—¶é—´ |
|--------|------|--------|--------|----------|
| 50 | 20åˆ†é’Ÿ | 5åˆ†é’Ÿ | 3åˆ†é’Ÿ | 15-17åˆ†é’Ÿ |
| 100 | 40åˆ†é’Ÿ | 10åˆ†é’Ÿ | 6åˆ†é’Ÿ | 30-34åˆ†é’Ÿ |
| 500 | 3.5å°æ—¶ | 50åˆ†é’Ÿ | 30åˆ†é’Ÿ | 2.5-3å°æ—¶ |
| 1000 | 7å°æ—¶ | 1.7å°æ—¶ | 1å°æ—¶ | **5-6å°æ—¶** |

---

## ğŸ›¡ï¸ å®‰å…¨å»ºè®®

### é¿å…è¢«å°IP

1. **ä¸è¦è®¾ç½®è¿‡é«˜çš„å¹¶å‘æ•°**
   ```bash
   # âŒ å±é™©ï¼šå®¹æ˜“è¢«å°
   --concurrency 100
   
   # âœ… å®‰å…¨ï¼šæ¨èèŒƒå›´
   --concurrency 10-20
   ```

2. **åˆ†æ‰¹çˆ¬å–**
   ```bash
   # æ¯æ‰¹200ä¸ªç½‘ç«™ï¼Œä¼‘æ¯5åˆ†é’Ÿ
   for i in {0..4}; do
       start=$((i * 200))
       end=$((start + 200))
       sed -n "${start},${end}p" data/large_urls.txt > data/batch_$i.txt
       
       python scripts/prepare_website_data.py \
         --urls-file data/batch_$i.txt \
         --output data/websites/batch_$i.jsonl \
         --concurrency 15
       
       echo "æ‰¹æ¬¡ $i å®Œæˆï¼Œä¼‘æ¯5åˆ†é’Ÿ..."
       sleep 300
   done
   ```

3. **ä½¿ç”¨ä»£ç†è½®æ¢**ï¼ˆé«˜çº§ï¼‰
   - å¯åœ¨ `WebsiteCrawler` ä¸­æ·»åŠ ä»£ç†æ± 
   - éšæœºUser-Agent
   - è¯·æ±‚é—´éšæœºå»¶è¿Ÿ

---

## ğŸ” ç›‘æ§ä¸è°ƒè¯•

### å®æ—¶ç›‘æ§

```bash
# ç»ˆç«¯1: è¿è¡Œçˆ¬è™«
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/output.jsonl \
  --concurrency 20

# ç»ˆç«¯2: ç›‘æ§è¿›åº¦
watch -n 5 "echo 'å·²çˆ¬å–:' && wc -l data/websites/output.jsonl"

# ç»ˆç«¯3: ç›‘æ§æ—¥å¿—
tail -f *.log
```

### æŸ¥çœ‹ç»Ÿè®¡

```bash
# æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡
cat data/websites/output.jsonl | python -c "
import json, sys
sites = [json.loads(l) for l in sys.stdin]
total_pages = sum(s.get('depth', 1) for s in sites)
frameworks = {}
for s in sites:
    fw = s.get('metadata', {}).get('framework', 'Unknown')
    frameworks[fw] = frameworks.get(fw, 0) + 1

print(f'ç½‘ç«™æ€»æ•°: {len(sites)}')
print(f'é¡µé¢æ€»æ•°: {total_pages}')
print(f'å¹³å‡æ·±åº¦: {total_pages/len(sites):.1f}')
print(f'\\næ¡†æ¶åˆ†å¸ƒ:')
for fw, count in sorted(frameworks.items(), key=lambda x: -x[1])[:10]:
    print(f'  {fw}: {count}')
"
```

---

## âœ… æœ€ä½³å®è·µ

### æ¨èå·¥ä½œæµ

```bash
#!/bin/bash
# é«˜æ•ˆçˆ¬å–1000ä¸ªç½‘ç«™çš„æœ€ä½³å®è·µ

cd /workspaces/BrowerAI/training

echo "=== æ­¥éª¤1: çˆ¬å–ç½‘ç«™ï¼ˆ2-3å°æ—¶ï¼‰==="
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/1000_sites.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20 \
  2>&1 | tee logs/crawl_1000.log

echo "=== æ­¥éª¤2: éªŒè¯æ•°æ® ==="
wc -l data/websites/1000_sites.jsonl

echo "=== æ­¥éª¤3: è®­ç»ƒæ¨¡å‹ï¼ˆ2-40å°æ—¶ï¼‰==="
python scripts/train_large_scale.py \
  --data-file data/websites/1000_sites.jsonl \
  --checkpoint-dir checkpoints/large_1000 \
  --epochs 50 \
  --batch-size 8 \
  2>&1 | tee logs/train_1000.log

echo "=== æ­¥éª¤4: æ¨ç†ç”Ÿæˆ ==="
python scripts/inference_website.py \
  --model checkpoints/large_1000/best_model.pt \
  --mode batch \
  --input data/websites/1000_sites.jsonl \
  --output results/1000_inference.json

echo "=== å®Œæˆï¼==="
```

---

## ğŸ‰ æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

âœ… **é€Ÿåº¦æå‡**: 5-10å€åŠ é€Ÿ  
âœ… **å¹¶å‘æ§åˆ¶**: Semaphoreé™æµ  
âœ… **é”™è¯¯å®¹é”™**: å•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“  
âœ… **å®æ—¶è¿›åº¦**: tqdmè¿›åº¦æ¡  
âœ… **çµæ´»é…ç½®**: å¯è°ƒæ•´å¹¶å‘æ•°

### ç«‹å³ä½¿ç”¨

```bash
# å¿«é€Ÿå¼€å§‹ - 1000ä¸ªç½‘ç«™ä»…éœ€2-3å°æ—¶ï¼
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/1000_sites.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20
```

**ç°åœ¨å°±å¼€å§‹å­¦ä¹ 1000ä¸ªç½‘ç«™ï¼ğŸš€**
