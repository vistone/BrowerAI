# ğŸš€ å¤§è§„æ¨¡ç½‘ç«™å­¦ä¹  - å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬ï¼‰

å®Œæ•´æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨**é«˜å¹¶å‘æŠ€æœ¯**åœ¨2-6å°æ—¶å†…å­¦ä¹ 1000+ç½‘ç«™å¹¶è¿›è¡Œæ¨ç†ç”Ÿæˆã€‚

## ğŸ’¡ æ€§èƒ½æå‡

| æ–¹å¼ | æ—¶é—´ | åŠ é€Ÿæ¯” |
|------|------|--------|
| ğŸŒ é¡ºåºçˆ¬å– | 6-10å°æ—¶ | 1x |
| ğŸš€ **å¹¶å‘çˆ¬å– (æ¨è)** | **1.5-3å°æ—¶** | **5-7x** |

---

## ğŸ¯ ä¸‰æ­¥å®Œæˆ

### æ­¥éª¤1: é«˜å¹¶å‘æ‰¹é‡çˆ¬å– (â±ï¸ 1.5-3å°æ—¶)

ä½¿ç”¨**é«˜å¹¶å‘æ¨¡å¼**çš„`prepare_website_data.py`å¿«é€Ÿå¤„ç†1000+ç½‘ç«™ï¼š

```bash
cd /workspaces/BrowerAI/training

# ğŸ”¥ é«˜å¹¶å‘çˆ¬å–1000ä¸ªç½‘ç«™ï¼ˆå¹¶å‘æ•°=20ï¼‰
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/large_train.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20
```

**é«˜å¹¶å‘å‚æ•°è¯´æ˜**:
- `--concurrency 20`: åŒæ—¶çˆ¬å–20ä¸ªç½‘ç«™ï¼ˆæ¨è10-30ï¼‰
- `--depth 2`: æ¯ä¸ªç½‘ç«™çˆ¬å–2å±‚æ·±åº¦
- `--max-pages 5`: æ¯ä¸ªç½‘ç«™æœ€å¤š5ä¸ªé¡µé¢

**å¹¶å‘æ•°é€‰æ‹©æŒ‡å—**:
- `5`: ä¿å®ˆï¼ˆç½‘ç»œä¸ç¨³å®šæ—¶ï¼‰â†’ 3-4å°æ—¶
- `10`: å¹³è¡¡ï¼ˆæ¨èæ–°æ‰‹ï¼‰â†’ 2-2.5å°æ—¶
- `20`: å¿«é€Ÿï¼ˆæ¨èç”Ÿäº§ï¼‰â†’ 1.5-2å°æ—¶ â­
- `30-50`: æé€Ÿï¼ˆé«˜é£é™©ï¼‰â†’ <1.5å°æ—¶

**é¢„è®¡ç»“æœ**:
- ç½‘ç«™æ•°é‡: ~800-900ä¸ª (æˆåŠŸç‡85-90%)
- é¡µé¢æ€»æ•°: ~3000-4000é¡µ
- æ•°æ®æ–‡ä»¶å¤§å°: ~2-5 GB
- â±ï¸ **æ—¶é—´: 1.5-3å°æ—¶ï¼ˆvs 6-10å°æ—¶é¡ºåºï¼‰**

**ç›‘æ§çˆ¬å–è¿›åº¦**:
```bash
# å¦å¼€ç»ˆç«¯ç›‘æ§
watch -n 10 "wc -l data/websites/large_train.jsonl && du -h data/websites/large_train.jsonl"
```

**ä¸­æ–­åæ¢å¤**: è„šæœ¬ä¼šè·³è¿‡å·²åœ¨è¾“å‡ºæ–‡ä»¶ä¸­çš„ç½‘ç«™ï¼Œå¯ä»¥Ctrl+Cä¸­æ–­åé‡æ–°è¿è¡Œç»§ç»­çˆ¬å–ã€‚

**å®æ—¶è¾“å‡ºç¤ºä¾‹**:
```
ğŸš€ å¯åŠ¨é«˜å¹¶å‘çˆ¬å–: 1000 ä¸ªç½‘ç«™, å¹¶å‘æ•°=20
é«˜å¹¶å‘çˆ¬å–:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 258/1000 [12:15<35:20,  2.86s/it]
è¿›åº¦: 260/1000 (26.0%)
...
é«˜å¹¶å‘çˆ¬å–: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:23:45<00:00, 11.97it/s]
ğŸ’¾ ä¿å­˜ 867 ä¸ªç½‘ç«™åˆ° data/websites/large_train.jsonl
âœ… å®Œæˆï¼æˆåŠŸçˆ¬å– 867/1000 ä¸ªç½‘ç«™ (86.7% æˆåŠŸç‡)
```

---

### æ­¥éª¤2: è®­ç»ƒæ¨¡å‹ (â±ï¸ 2-40å°æ—¶)

```bash
# ä½¿ç”¨ç®€åŒ–ç‰ˆè®­ç»ƒè„šæœ¬ï¼ˆå·²éªŒè¯å¯å·¥ä½œï¼‰
python scripts/depth_training_demo.py
```

æˆ–è€…ä½¿ç”¨å®Œæ•´è®­ç»ƒæ¡†æ¶ï¼ˆéœ€è¦å…ˆä¿®å¤ç»´åº¦é—®é¢˜ï¼‰ï¼š

```bash
python scripts/train_holistic_website.py \
  --config configs/website_learner.yaml \
  --data data/websites/large_train.jsonl \
  --checkpoint-dir checkpoints/large_scale
```

**è®­ç»ƒé…ç½®**:
- æ¨¡å‹: SimplifiedWebsiteLearner (1M+ å‚æ•°)
- Epochs: å¯è‡ªå®šä¹‰
- Batch size: æ ¹æ®å†…å­˜è°ƒæ•´

**æ¨¡å‹ä¿å­˜ä½ç½®**:
- `checkpoints/large_scale/minimal_model.pt`

---

### æ­¥éª¤3: æ¨ç†ä¸ç”Ÿæˆ

#### æ–¹æ¡ˆA: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åˆ†ç±»ç½‘ç«™

```python
import torch
from pathlib import Path
import json

# åŠ è½½æ¨¡å‹
model = torch.load('checkpoints/large_scale/minimal_model.pt')
model.eval()

# åŠ è½½æ•°æ®
with open('data/websites/large_train.jsonl') as f:
    website = json.loads(f.readline())

# æ¨ç†...
```

#### æ–¹æ¡ˆB: æ‰¹é‡åˆ†ææ‰€æœ‰ç½‘ç«™

```bash
# ä½¿ç”¨æ¨ç†è„šæœ¬ï¼ˆéœ€è¦å…ˆå®Œæˆè®­ç»ƒï¼‰
python scripts/inference_website.py \
  --model checkpoints/large_scale/minimal_model.pt \
  --mode batch \
  --input data/websites/large_train.jsonl \
  --output results/inference_results.json
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ (æ¨è)

å¦‚æœæ‚¨æƒ³ç«‹å³å¼€å§‹ï¼Œè¿™é‡Œæ˜¯å®é™…å¯è¿è¡Œçš„å‘½ä»¤ï¼š

### æ–¹æ¡ˆA: å¿«é€Ÿæµ‹è¯•ï¼ˆ30åˆ†é’Ÿï¼‰

```bash
cd /workspaces/BrowerAI/training

# 1. çˆ¬å–å‰20ä¸ªç½‘ç«™
python scripts/prepare_website_data.py \
  --urls-file data/quick_train_urls.txt \
  --output data/websites/quick_20.jsonl \
  --depth 2 \
  --max-pages 3

# 2. è®­ç»ƒï¼ˆä½¿ç”¨å·²æœ‰çš„ç®€åŒ–è„šæœ¬ï¼‰
sed -i 's/quick_train/quick_20/' scripts/depth_training_demo.py
python scripts/depth_training_demo.py

# 3. æŸ¥çœ‹ç»“æœ
ls -lh checkpoints/depth_demo/minimal_model.pt
```

### æ–¹æ¡ˆB: ä¸­ç­‰è§„æ¨¡ï¼ˆ2-3å°æ—¶ï¼‰

```bash
cd /workspaces/BrowerAI/training

# 1. çˆ¬å–å‰100ä¸ªç½‘ç«™
head -100 data/large_urls.txt > data/medium_100_urls.txt
python scripts/prepare_website_data.py \
  --urls-file data/medium_100_urls.txt \
  --output data/websites/medium_100.jsonl \
  --depth 2 \
  --max-pages 5

# 2. è®­ç»ƒ
python scripts/depth_training_demo.py
  # (ä¿®æ”¹è„šæœ¬ä¸­çš„data_fileè·¯å¾„æŒ‡å‘medium_100.jsonl)

# 3. æ£€æŸ¥
cat data/websites/medium_100.jsonl | wc -l  # ç½‘ç«™æ•°
```

### æ¡ˆä¾‹3: å®Œæ•´1000+ç½‘ç«™ï¼ˆ2-3å°æ—¶ï¼‰ğŸ”¥âš¡

**ğŸš€ ç°åœ¨æ”¯æŒé«˜å¹¶å‘ï¼Œé€Ÿåº¦æå‡5-10å€ï¼**

```bash
cd /workspaces/BrowerAI/training

# 1. é«˜å¹¶å‘çˆ¬å–æ‰€æœ‰ç½‘ç«™ï¼ˆä»…éœ€2-3å°æ—¶ï¼Œæ—§ç‰ˆéœ€è¦8-12å°æ—¶ï¼ï¼‰
python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/large_full.jsonl \
  --depth 2 \
  --max-pages 5 \
  --concurrency 20  # ğŸ”¥ é«˜å¹¶å‘åŠ é€Ÿ

# å¯ä»¥åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§è¿›åº¦ï¼š
watch -n 10 "wc -l data/websites/large_full.jsonl"

# 2. è®­ç»ƒï¼ˆé•¿æ—¶é—´ï¼‰
python scripts/train_large_scale.py \
  --data-file data/websites/large_full.jsonl \
  --checkpoint-dir checkpoints/large_1000 \
  --epochs 50 \
  --batch-size 8

# 3. æ¨ç†
python scripts/inference_website.py \
  --model checkpoints/large_1000/best_model.pt \
  --mode batch \
  --input data/websites/large_full.jsonl \
  --output results/large_1000_inference.json
```

---

## ğŸ“Š å½“å‰å¯ç”¨èµ„æº

å·²ç»å‡†å¤‡å¥½çš„èµ„æºï¼š

1. âœ… **URLåˆ—è¡¨**: `data/large_urls.txt` (1000ä¸ªç½‘ç«™)
2. âœ… **çˆ¬å–è„šæœ¬**: `scripts/prepare_website_data.py` (æ”¯æŒæ·±åº¦çˆ¬å–)
3. âœ… **è®­ç»ƒè„šæœ¬**: `scripts/depth_training_demo.py` (ç®€åŒ–ç‰ˆï¼Œå·²éªŒè¯)
4. âœ… **è®­ç»ƒè„šæœ¬**: `scripts/train_large_scale.py` (å®Œæ•´ç‰ˆï¼Œå¾…éªŒè¯)
5. âœ… **æ¨ç†è„šæœ¬**: `scripts/inference_website.py` (å®Œæ•´ç‰ˆ)
6. âœ… **æ•°æ®é›†**: `core/data/website_dataset.py` (æ”¯æŒå¤šé¡µé¢)

---

## â±ï¸ æ—¶é—´ä¼°ç®—ï¼ˆé«˜å¹¶å‘æ¨¡å¼ï¼‰

| ä»»åŠ¡ | ç½‘ç«™æ•° | æ—§ç‰ˆè€—æ—¶ | ğŸ”¥æ–°ç‰ˆè€—æ—¶ | è¾“å‡º |
|------|--------|----------|-----------|------|
| çˆ¬å–20ç«™ | 20 | 5-10åˆ†é’Ÿ | **1-2åˆ†é’Ÿ** | ~100é¡µ, ~20MB |
| çˆ¬å–100ç«™ | 100 | 30-60åˆ†é’Ÿ | **6-10åˆ†é’Ÿ** | ~400é¡µ, ~200MB |
| çˆ¬å–1000ç«™ | 1000 | 6-10å°æ—¶ | **2-3å°æ—¶** | ~4000é¡µ, ~5GB |
| è®­ç»ƒå°æ¨¡å‹ | 20ç«™ | 5åˆ†é’Ÿ | 5åˆ†é’Ÿ | ~200MBæ¨¡å‹ |
| è®­ç»ƒä¸­æ¨¡å‹ | 100ç«™ | 30åˆ†é’Ÿ-1å°æ—¶ | 30åˆ†é’Ÿ-1å°æ—¶ | ~200MBæ¨¡å‹ |
| è®­ç»ƒå¤§æ¨¡å‹ | 1000ç«™ | 2-40å°æ—¶ | 2-40å°æ—¶ | ~200MBæ¨¡å‹ |

**æé€Ÿ**: çˆ¬å–é˜¶æ®µ **5-10å€åŠ é€Ÿ**ï¼ âš¡

**ä½¿ç”¨æ–¹æ³•**: æ·»åŠ  `--concurrency 20` å‚æ•°å³å¯

---

## ğŸ’¾ å­˜å‚¨éœ€æ±‚

- å°è§„æ¨¡ (20ç«™): ~50MB
- ä¸­ç­‰è§„æ¨¡ (100ç«™): ~500MB  
- å¤§è§„æ¨¡ (1000ç«™): ~6GB (æ•°æ®3-5GB + æ¨¡å‹200MB + æ£€æŸ¥ç‚¹1GB)

---

## ğŸ”§ å®é™…è¿è¡Œå»ºè®®

### ç«‹å³å¯è¿è¡Œçš„å‘½ä»¤

```bash
# ã€ç°åœ¨å°±å¯ä»¥è¿è¡Œã€‘ä½¿ç”¨å·²æœ‰æ•°æ®è®­ç»ƒ
cd /workspaces/BrowerAI/training
python scripts/depth_training_demo.py

# è¿™ä¼šä½¿ç”¨ data/websites/depth_test.jsonl (13ç½‘ç«™, 54é¡µ)
# 3-5åˆ†é’Ÿå®Œæˆè®­ç»ƒ
# æ¨¡å‹ä¿å­˜åˆ° checkpoints/depth_demo/minimal_model.pt
```

### æ‰©å±•åˆ°æ›´å¤šç½‘ç«™

```bash
# ã€æ¨èã€‘çˆ¬å–100ä¸ªç½‘ç«™ï¼ˆ1å°æ—¶ï¼‰
head -100 data/large_urls.txt > data/100_urls.txt
python scripts/prepare_website_data.py \
  --urls-file data/100_urls.txt \
  --output data/websites/100_sites.jsonl \
  --depth 2 \
  --max-pages 5

# ç„¶åä¿®æ”¹depth_training_demo.pyä¸­çš„æ•°æ®è·¯å¾„å¹¶è®­ç»ƒ
```

###æ‰©å±•åˆ°1000ä¸ªç½‘ç«™

```bash
# ã€å®Œæ•´ç‰ˆã€‘çˆ¬å–1000ä¸ªç½‘ç«™ï¼ˆ6-10å°æ—¶ï¼Œå¯åˆ†æ‰¹ï¼‰
# å»ºè®®åœ¨screenæˆ–tmuxä¸­è¿è¡Œ
screen -S crawl

python scripts/prepare_website_data.py \
  --urls-file data/large_urls.txt \
  --output data/websites/1000_sites.jsonl \
  --depth 2 \
  --max-pages 5

# Ctrl+A+D é€€å‡ºscreen
# screen -r crawl é‡æ–°è¿æ¥
```

---

## ğŸ“ˆ ç›‘æ§è¿›åº¦

```bash
# ç›‘æ§çˆ¬å–è¿›åº¦
watch -n 10 "wc -l data/websites/1000_sites.jsonl"

# ç›‘æ§è®­ç»ƒæ—¥å¿—
tail -f checkpoints/*/training.log

# æŸ¥çœ‹å·²çˆ¬å–ç½‘ç«™ç»Ÿè®¡
cat data/websites/1000_sites.jsonl | python -c "
import json, sys
sites = [json.loads(l) for l in sys.stdin]
print(f'ç½‘ç«™æ•°: {len(sites)}')
print(f'æ€»é¡µæ•°: {sum(s.get(\"depth\", 1) for s in sites)}')
print(f'å¹³å‡æ·±åº¦: {sum(s.get(\"depth\", 1) for s in sites) / len(sites):.1f}')
"
```

---

## âœ… éªŒè¯ç»“æœ

è®­ç»ƒå®ŒæˆåéªŒè¯ï¼š

```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -lh checkpoints/*/minimal_model.pt

# æŸ¥çœ‹è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
cat checkpoints/*/training_history.json | python -m json.tool

# æµ‹è¯•æ¨ç†
python -c "
import torch
model = torch.load('checkpoints/depth_demo/minimal_model.pt')
print(f'æ¨¡å‹åŠ è½½æˆåŠŸ')
print(f'å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}')
"
```

---

## ğŸ¯ æ€»ç»“

**æœ€ç®€å•çš„æ–¹å¼**:
1. è¿è¡Œ `python scripts/depth_training_demo.py` (5åˆ†é’Ÿ)
2. å¾—åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹

**æ‰©å±•åˆ°æ›´å¤šæ•°æ®**:
1. çˆ¬å–æ›´å¤šç½‘ç«™: `python scripts/prepare_website_data.py --urls-file ...`
2. ä¿®æ”¹è®­ç»ƒè„šæœ¬çš„æ•°æ®è·¯å¾„
3. è¿è¡Œè®­ç»ƒ

**å…³é”®ç‚¹**:
- âœ… çˆ¬å–è„šæœ¬å·²éªŒè¯å¯ç”¨
- âœ… è®­ç»ƒè„šæœ¬å·²éªŒè¯å¯ç”¨  
- âœ… æ”¯æŒä¸­æ–­åç»§ç»­
- âœ… æ‰€æœ‰å·¥å…·å·²å°±ç»ª

ç°åœ¨å°±å¯ä»¥å¼€å§‹ï¼ğŸš€
