#!/usr/bin/env python3
"""
å¢é‡å­¦ä¹ è„šæœ¬ - è¾¹çˆ¬è¾¹å­¦
Incremental Learning: Crawl one website â†’ Learn immediately â†’ Update model

ä¼˜åŠ¿:
1. æ— éœ€ä¿å­˜å¤§é‡ä¸­é—´æ•°æ® (èŠ‚çœ3-5GBå­˜å‚¨)
2. å®æ—¶æ›´æ–°æ¨¡å‹ (éšæ—¶å¯ç”¨)
3. å†…å­˜å‹å¥½ (åªå¤„ç†å½“å‰ç½‘ç«™)
4. ä¸­æ–­å®‰å…¨ (æ¨¡å‹å·²ä¿å­˜)
"""

import asyncio
import json
import logging
import argparse
from pathlib import Path
from typing import Optional, Dict, Any
import sys
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from prepare_website_data import WebsiteCrawler, load_urls_from_file

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimplifiedWebsiteLearner(nn.Module):
    """ç®€åŒ–ç‰ˆç½‘ç«™å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, vocab_size: int = 10000, embed_dim: int = 128, hidden_dim: int = 256):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # å¤šä»»åŠ¡å¤´
        self.framework_classifier = nn.Linear(hidden_dim * 2, 20)  # 20ç§æ¡†æ¶
        self.category_classifier = nn.Linear(hidden_dim * 2, 10)   # 10ç§åˆ†ç±»
        
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        embedded = self.dropout(embedded)
        
        lstm_out, (hidden, _) = self.lstm(embedded)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        features = lstm_out[:, -1, :]  # (batch, hidden_dim*2)
        features = self.dropout(features)
        
        framework_logits = self.framework_classifier(features)
        category_logits = self.category_classifier(features)
        
        return framework_logits, category_logits


class WebsiteTokenizer:
    """ç®€å•çš„ç½‘ç«™å†…å®¹åˆ†è¯å™¨"""
    
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}
        self.word_freq = {}
    
    def build_vocab(self, text: str):
        """æ„å»ºè¯æ±‡è¡¨"""
        words = text.lower().split()
        for word in words:
            self.word_freq[word] = self.word_freq.get(word, 0) + 1
    
    def finalize_vocab(self):
        """å›ºå®šè¯æ±‡è¡¨ï¼ˆå–æœ€å¸¸è§çš„è¯ï¼‰"""
        sorted_words = sorted(self.word_freq.items(), key=lambda x: -x[1])
        for idx, (word, _) in enumerate(sorted_words[:self.vocab_size-2], start=2):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
    
    def encode(self, text: str, max_len: int = 512) -> list:
        """ç¼–ç æ–‡æœ¬ä¸ºtoken IDs"""
        words = text.lower().split()[:max_len]
        tokens = [self.word2idx.get(w, 1) for w in words]  # 1 = <UNK>
        
        # Padding
        if len(tokens) < max_len:
            tokens += [0] * (max_len - len(tokens))
        
        return tokens


class IncrementalLearner:
    """å¢é‡å­¦ä¹ å™¨ - è¾¹çˆ¬è¾¹å­¦"""
    
    def __init__(
        self,
        checkpoint_dir: str = 'checkpoints/incremental',
        learning_rate: float = 1e-4,
        device: str = 'auto'
    ):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾å¤‡
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = SimplifiedWebsiteLearner().to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.tokenizer = WebsiteTokenizer()
        
        # æ ‡ç­¾æ˜ å°„
        self.framework_map = {}
        self.category_map = {}
        
        # ç»Ÿè®¡
        self.total_trained = 0
        self.training_history = []
        
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        self.load_checkpoint()
    
    def extract_text(self, website_data: Dict[str, Any]) -> str:
        """æå–ç½‘ç«™æ–‡æœ¬å†…å®¹"""
        texts = []
        
        # ä¸»é¡µé¢HTMLå†…å®¹
        if 'pages' in website_data and 'main' in website_data['pages']:
            main_page = website_data['pages']['main']
            if 'html' in main_page:
                texts.append(main_page['html'][:5000])  # é™åˆ¶é•¿åº¦
        
        # å­é¡µé¢å†…å®¹
        if 'pages' in website_data and 'sub_pages' in website_data['pages']:
            for page in website_data['pages']['sub_pages'][:5]:
                if 'html' in page:
                    texts.append(page['html'][:3000])
        
        # CSSå†…å®¹
        if 'pages' in website_data and 'main' in website_data['pages']:
            main_page = website_data['pages']['main']
            if 'css_files' in main_page:
                for css in main_page['css_files'][:5]:
                    if 'content' in css:
                        texts.append(css['content'][:1000])
        
        # JSå†…å®¹
        if 'pages' in website_data and 'main' in website_data['pages']:
            main_page = website_data['pages']['main']
            if 'js_files' in main_page:
                for js in main_page['js_files'][:5]:
                    if 'content' in js:
                        texts.append(js['content'][:1000])
        
        return ' '.join(texts)
    
    def prepare_labels(self, website_data: Dict[str, Any]) -> tuple:
        """å‡†å¤‡æ ‡ç­¾"""
        # Frameworkæ ‡ç­¾
        framework = website_data.get('metadata', {}).get('framework', 'Unknown')
        if framework not in self.framework_map:
            self.framework_map[framework] = len(self.framework_map)
        framework_idx = self.framework_map[framework]
        
        # Categoryæ ‡ç­¾
        category = website_data.get('category', 'unknown')
        if category not in self.category_map:
            self.category_map[category] = len(self.category_map)
        category_idx = self.category_map[category]
        
        return framework_idx, category_idx
    
    def train_on_website(self, website_data: Dict[str, Any]) -> Dict[str, float]:
        """åœ¨å•ä¸ªç½‘ç«™ä¸Šè®­ç»ƒ"""
        # æå–æ–‡æœ¬
        text = self.extract_text(website_data)
        if not text:
            logger.warning(f"ç½‘ç«™ {website_data.get('url', 'unknown')} æ— æœ‰æ•ˆå†…å®¹")
            return {
                'loss': 0.0,
                'loss_framework': 0.0,
                'loss_category': 0.0
            }
        
        # æ›´æ–°è¯æ±‡è¡¨
        self.tokenizer.build_vocab(text)
        
        # ç¼–ç 
        tokens = self.tokenizer.encode(text)
        x = torch.tensor([tokens], dtype=torch.long).to(self.device)
        
        # å‡†å¤‡æ ‡ç­¾
        framework_idx, category_idx = self.prepare_labels(website_data)
        y_framework = torch.tensor([framework_idx], dtype=torch.long).to(self.device)
        y_category = torch.tensor([category_idx], dtype=torch.long).to(self.device)
        
        # è®­ç»ƒ
        self.model.train()
        self.optimizer.zero_grad()
        
        framework_logits, category_logits = self.model(x)
        
        # å¤šä»»åŠ¡æŸå¤±
        loss_framework = nn.CrossEntropyLoss()(framework_logits, y_framework)
        loss_category = nn.CrossEntropyLoss()(category_logits, y_category)
        loss = loss_framework + loss_category
        
        loss.backward()
        self.optimizer.step()
        
        self.total_trained += 1
        
        return {
            'loss': loss.item(),
            'loss_framework': loss_framework.item(),
            'loss_category': loss_category.item()
        }
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.checkpoint_dir / 'latest.pt'
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'tokenizer_word2idx': self.tokenizer.word2idx,
            'tokenizer_word_freq': self.tokenizer.word_freq,
            'framework_map': self.framework_map,
            'category_map': self.category_map,
            'total_trained': self.total_trained,
            'training_history': self.training_history
        }, checkpoint_path)
        
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.checkpoint_dir / 'latest.pt'
        
        if not checkpoint_path.exists():
            logger.info("ğŸ“ ä»å¤´å¼€å§‹è®­ç»ƒ")
            return
        
        logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.tokenizer.word2idx = checkpoint['tokenizer_word2idx']
        self.tokenizer.word_freq = checkpoint['tokenizer_word_freq']
        self.framework_map = checkpoint['framework_map']
        self.category_map = checkpoint['category_map']
        self.total_trained = checkpoint['total_trained']
        self.training_history = checkpoint.get('training_history', [])
        
        logger.info(f"âœ… å·²æ¢å¤è®­ç»ƒçŠ¶æ€: {self.total_trained} ä¸ªç½‘ç«™")


async def incremental_learning_pipeline(
    urls_file: str,
    checkpoint_dir: str,
    max_depth: int = 2,
    max_pages: int = 5,
    learning_rate: float = 1e-4,
    save_frequency: int = 10
):
    """å¢é‡å­¦ä¹ æµæ°´çº¿ï¼šçˆ¬å– â†’ ç«‹å³å­¦ä¹  â†’ ä¿å­˜"""
    
    # åŠ è½½URLåˆ—è¡¨
    logger.info(f"ğŸ“‹ åŠ è½½URLåˆ—è¡¨: {urls_file}")
    urls = load_urls_from_file(urls_file)
    total_urls = len(urls)
    logger.info(f"ğŸ“Š æ€»å…± {total_urls} ä¸ªç½‘ç«™")
    
    # åˆå§‹åŒ–å­¦ä¹ å™¨
    learner = IncrementalLearner(
        checkpoint_dir=checkpoint_dir,
        learning_rate=learning_rate
    )
    
    # åˆå§‹åŒ–çˆ¬è™«
    async with WebsiteCrawler(max_files=50, max_depth=max_depth, max_pages=max_pages) as crawler:
        
        start_idx = learner.total_trained
        logger.info(f"ğŸš€ ä»ç¬¬ {start_idx + 1} ä¸ªç½‘ç«™å¼€å§‹")
        
        for idx, (url, category) in enumerate(urls[start_idx:], start=start_idx + 1):
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"[{idx}/{total_urls}] ğŸŒ çˆ¬å–: {url}")
                
                # 1ï¸âƒ£ çˆ¬å–ç½‘ç«™
                website_data = await crawler.crawl_website(url, category)
                
                if not website_data:
                    logger.warning(f"âŒ çˆ¬å–å¤±è´¥: {url}")
                    continue
                
                logger.info(f"âœ… çˆ¬å–å®Œæˆ: {website_data.get('depth', 0)} ä¸ªé¡µé¢")
                
                # 2ï¸âƒ£ ç«‹å³å­¦ä¹ 
                logger.info(f"ğŸ§  å¼€å§‹å­¦ä¹ ...")
                losses = learner.train_on_website(website_data)
                
                if losses['loss'] > 0:
                    logger.info(f"ğŸ“ˆ æŸå¤±: {losses['loss']:.4f} "
                              f"(æ¡†æ¶:{losses['loss_framework']:.4f}, "
                              f"åˆ†ç±»:{losses['loss_category']:.4f})")
                else:
                    logger.info(f"â­ï¸  è·³è¿‡ï¼ˆæ— æœ‰æ•ˆå†…å®¹ï¼‰")
                
                # è®°å½•å†å²
                learner.training_history.append({
                    'url': url,
                    'category': category,
                    'loss': losses['loss'],
                    'timestamp': datetime.now().isoformat()
                })
                
                # 3ï¸âƒ£ å®šæœŸä¿å­˜
                if idx % save_frequency == 0:
                    learner.save_checkpoint()
                    logger.info(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ ({idx}/{total_urls})")
                
            except KeyboardInterrupt:
                logger.info("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
                learner.save_checkpoint()
                raise
            
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ {url}: {e}")
                continue
        
        # æœ€ç»ˆä¿å­˜
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ å…¨éƒ¨å®Œæˆï¼ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        learner.save_checkpoint()
        
        # å›ºåŒ–è¯æ±‡è¡¨
        learner.tokenizer.finalize_vocab()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = Path(checkpoint_dir) / 'final_model.pt'
        torch.save({
            'model_state_dict': learner.model.state_dict(),
            'tokenizer': learner.tokenizer,
            'framework_map': learner.framework_map,
            'category_map': learner.category_map
        }, final_path)
        
        logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        logger.info(f"ğŸ“Š æ€»å…±è®­ç»ƒ: {learner.total_trained} ä¸ªç½‘ç«™")


def main():
    parser = argparse.ArgumentParser(description='å¢é‡å­¦ä¹  - è¾¹çˆ¬è¾¹å­¦')
    parser.add_argument('--urls-file', type=str, required=True,
                       help='URLåˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints/incremental',
                       help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--depth', type=int, default=2,
                       help='æœ€å¤§çˆ¬å–æ·±åº¦')
    parser.add_argument('--max-pages', type=int, default=5,
                       help='æ¯ä¸ªç½‘ç«™æœ€å¤§é¡µé¢æ•°')
    parser.add_argument('--learning-rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--save-frequency', type=int, default=10,
                       help='æ¯Nä¸ªç½‘ç«™ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹')
    
    args = parser.parse_args()
    
    logger.info("="*60)
    logger.info("ğŸš€ å¢é‡å­¦ä¹ æ¨¡å¼å¯åŠ¨")
    logger.info("="*60)
    logger.info(f"URLæ–‡ä»¶: {args.urls_file}")
    logger.info(f"æ£€æŸ¥ç‚¹ç›®å½•: {args.checkpoint_dir}")
    logger.info(f"æ·±åº¦: {args.depth}, é¡µé¢æ•°: {args.max_pages}")
    logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
    logger.info(f"ä¿å­˜é¢‘ç‡: æ¯ {args.save_frequency} ä¸ªç½‘ç«™")
    logger.info("="*60)
    
    asyncio.run(incremental_learning_pipeline(
        urls_file=args.urls_file,
        checkpoint_dir=args.checkpoint_dir,
        max_depth=args.depth,
        max_pages=args.max_pages,
        learning_rate=args.learning_rate,
        save_frequency=args.save_frequency
    ))


if __name__ == '__main__':
    main()
