#!/usr/bin/env python3
"""
å¤§è§„æ¨¡ç½‘ç«™å­¦ä¹ è®­ç»ƒè„šæœ¬

è®­ç»ƒ1000+ç½‘ç«™çš„å®Œæ•´æ¨¡å‹ï¼Œæ”¯æŒ:
- å¤§æ•°æ®é›†è®­ç»ƒ
- æ£€æŸ¥ç‚¹ä¿å­˜
- è®­ç»ƒæ¢å¤
- æ¨¡å‹å¯¼å‡ºONNX
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from pathlib import Path
import json
import logging
from datetime import datetime
from tqdm import tqdm
import sys

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.data.website_dataset import WebsiteDataset
from core.data.tokenizers import CodeTokenizer
from core.models.website_learner import HolisticWebsiteLearner

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LargeScaleTrainer:
    """å¤§è§„æ¨¡è®­ç»ƒå™¨"""
    
    def __init__(self, config: dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.checkpoint_dir = Path(config['checkpoint_dir'])
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ–¥ï¸  è®­ç»ƒè®¾å¤‡: {self.device}")
        logger.info(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: {self.checkpoint_dir}")
    
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®é›†"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®é›†...")
        
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = CodeTokenizer(vocab_size=config['vocab_size'])
        
        # åŠ è½½æ•°æ®é›†
        dataset = WebsiteDataset(
            data_file=Path(config['data_file']),
            tokenizer=tokenizer,
            max_html_len=config['max_html_len'],
            max_css_len=config['max_css_len'],
            max_js_len=config['max_js_len']
        )
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        logger.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
        logger.info(f"  - è®­ç»ƒé›†: {train_size} ä¸ªç½‘ç«™")
        logger.info(f"  - éªŒè¯é›†: {val_size} ä¸ªç½‘ç«™")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=config.get('num_workers', 0),
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=config.get('num_workers', 0)
        )
        
        return train_loader, val_loader, tokenizer
    
    def create_model(self, tokenizer):
        """åˆ›å»ºæ¨¡å‹"""
        logger.info("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        
        model = HolisticWebsiteLearner(
            vocab_size=config['vocab_size'],
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            num_categories=len(WebsiteDataset.CATEGORIES),
            url_feature_dim=128
        )
        
        model = model.to(self.device)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ:")
        logger.info(f"  - æ€»å‚æ•°: {total_params:,}")
        logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"  - æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB")
        
        return model
    
    def load_checkpoint(self, model, optimizer, scheduler):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / "latest_checkpoint.pt"
        
        if checkpoint_file.exists():
            logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_file}")
            checkpoint = torch.load(checkpoint_file, map_location=self.device)
            
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            
            logger.info(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ (ä»epoch {start_epoch}ç»§ç»­)")
            return start_epoch, best_val_loss
        
        return 0, float('inf')
    
    def save_checkpoint(self, model, optimizer, scheduler, epoch, val_loss, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'val_loss': val_loss,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_file = self.checkpoint_dir / "latest_checkpoint.pt"
        torch.save(checkpoint, latest_file)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_file = self.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_file)
            logger.info(f"ğŸ’ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_file}")
        
        # å®šæœŸä¿å­˜epochæ£€æŸ¥ç‚¹
        if (epoch + 1) % config.get('save_interval', 10) == 0:
            epoch_file = self.checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"
            torch.save(checkpoint, epoch_file)
    
    def train_epoch(self, model, train_loader, optimizer, criterion, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for batch_idx, batch in enumerate(pbar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            html_ids = batch['html_ids'].to(self.device)
            css_ids = batch['css_ids'].to(self.device)
            js_ids = batch['js_ids'].to(self.device)
            url_features = batch['url_features'].to(self.device)
            category = batch['category'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(html_ids, css_ids, js_ids, url_features)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs['category_logits'], category)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = outputs['category_logits'].max(1)
            total += category.size(0)
            correct += predicted.eq(category).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'acc': f"{100. * correct / total:.2f}%"
            })
        
        avg_loss = total_loss / len(train_loader)
        acc = 100. * correct / total
        
        return avg_loss, acc
    
    def validate(self, model, val_loader, criterion):
        """éªŒè¯"""
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯"):
                html_ids = batch['html_ids'].to(self.device)
                css_ids = batch['css_ids'].to(self.device)
                js_ids = batch['js_ids'].to(self.device)
                url_features = batch['url_features'].to(self.device)
                category = batch['category'].to(self.device)
                
                outputs = model(html_ids, css_ids, js_ids, url_features)
                loss = criterion(outputs['category_logits'], category)
                
                total_loss += loss.item()
                _, predicted = outputs['category_logits'].max(1)
                total += category.size(0)
                correct += predicted.eq(category).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        acc = 100. * correct / total
        
        return avg_loss, acc
    
    def export_onnx(self, model, tokenizer):
        """å¯¼å‡ºONNXæ¨¡å‹"""
        logger.info("ğŸ“¦ å¯¼å‡ºONNXæ¨¡å‹...")
        
        model.eval()
        model.to('cpu')
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        batch_size = 1
        html_ids = torch.randint(0, config['vocab_size'], (batch_size, config['max_html_len']))
        css_ids = torch.randint(0, config['vocab_size'], (batch_size, config['max_css_len']))
        js_ids = torch.randint(0, config['vocab_size'], (batch_size, config['max_js_len']))
        url_features = torch.randn(batch_size, 128)
        
        # å¯¼å‡º
        onnx_file = self.checkpoint_dir / "website_learner.onnx"
        torch.onnx.export(
            model,
            (html_ids, css_ids, js_ids, url_features),
            onnx_file,
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['html_ids', 'css_ids', 'js_ids', 'url_features'],
            output_names=['category_logits', 'framework_logits', 'style_embedding'],
            dynamic_axes={
                'html_ids': {0: 'batch_size'},
                'css_ids': {0: 'batch_size'},
                'js_ids': {0: 'batch_size'},
                'url_features': {0: 'batch_size'}
            }
        )
        
        logger.info(f"âœ… ONNXæ¨¡å‹å·²å¯¼å‡º: {onnx_file}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ å¼€å§‹å¤§è§„æ¨¡ç½‘ç«™å­¦ä¹ è®­ç»ƒ")
        logger.info("="*60 + "\n")
        
        # å‡†å¤‡æ•°æ®
        train_loader, val_loader, tokenizer = self.prepare_data()
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model(tokenizer)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config['epochs']
        )
        
        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        start_epoch, best_val_loss = self.load_checkpoint(model, optimizer, scheduler)
        
        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        logger.info(f"\nğŸ“ˆ å¼€å§‹è®­ç»ƒ (epochs: {start_epoch} -> {config['epochs']})\n")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(start_epoch, config['epochs']):
            logger.info(f"\n{'='*60}")
            logger.info(f"Epoch {epoch+1}/{config['epochs']}")
            logger.info(f"{'='*60}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(
                model, train_loader, optimizer, criterion, epoch
            )
            
            # éªŒè¯
            val_loss, val_acc = self.validate(model, val_loader, criterion)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            # æ‰“å°ç»“æœ
            logger.info(f"\nè®­ç»ƒ: loss={train_loss:.4f}, acc={train_acc:.2f}%")
            logger.info(f"éªŒè¯: loss={val_loss:.4f}, acc={val_acc:.2f}%")
            logger.info(f"å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
            
            self.save_checkpoint(model, optimizer, scheduler, epoch, val_loss, is_best)
            
            # ä¿å­˜å†å²
            history_file = self.checkpoint_dir / "training_history.json"
            with open(history_file, 'w') as f:
                json.dump(history, f, indent=2)
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        logger.info("="*60 + "\n")
        
        # å¯¼å‡ºONNX
        self.export_onnx(model, tokenizer)
        
        return model, history


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤§è§„æ¨¡ç½‘ç«™å­¦ä¹ è®­ç»ƒ")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-file", type=Path, default=Path("data/websites/large_train.jsonl"))
    parser.add_argument("--checkpoint-dir", type=Path, default=Path("checkpoints/large_scale"))
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    parser.add_argument("--resume", action="store_true", help="ä»æ£€æŸ¥ç‚¹æ¢å¤")
    
    args = parser.parse_args()
    
    # é…ç½®
    config = {
        'data_file': str(args.data_file),
        'checkpoint_dir': str(args.checkpoint_dir),
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'vocab_size': 10000,
        'd_model': 512,
        'nhead': 8,
        'num_layers': 6,
        'max_html_len': 2048,
        'max_css_len': 1024,
        'max_js_len': 2048,
        'num_workers': 4,
        'weight_decay': 0.01,
        'save_interval': 5
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LargeScaleTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    model, history = trainer.train()
    
    logger.info("\nâœ¨ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
